{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Administrative Outcomes Predictive Model\n",
    "\n",
    "Data Source: https://physionet.org/content/mimic-iv-fhir/1.0/\n",
    "\n",
    "- [Methods for De-identification of PHI | Health and Human Services](https://www.hhs.gov/hipaa/for-professionals/special-topics/de-identification/index.html)\n",
    "- [General Equivalence Mappings GEMS](https://society.asco.org/practice-policy/billing-coding-reporting/icd-10/general-equivalence-mappings-gems)\n",
    "\n",
    "https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings Download link\n",
    "\n",
    "ICD 10 is the latest, convert ICD to 9\n",
    "ICD 10 Code first three character defines a Category Codes\n",
    "\n",
    "**FHIR Resources**\n",
    "- Patient: For demographic information.\n",
    "- Encounter: To track patient visits and interactions.\n",
    "- Condition: For diagnoses and health conditions.\n",
    "- Procedure: For medical procedures performed.\n",
    "- Observation: For lab results and vital signs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Project Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FhirDataApplication\").getOrCreate()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Patient\n",
    "- `patientID`: Unique identifier for the patient\n",
    "- `gender`: Gender of the patient (male/female)\n",
    "- `birthDate`: Birth Date\n",
    "- `maritalStatus`: Marital Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Patient.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicPatient =  spark.read.schema(schema).json(\"_dataset/MimicPatient.ndjson\")\n",
    "    \n",
    "df_MimicPatient.createOrReplaceTempView(\"df_patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_patient = spark.sql(\"\"\"\n",
    "select \n",
    "    id AS patientId,\n",
    "    gender AS gender,\n",
    "    to_date(birthDate) birthDate,\n",
    "    maritalStatus.coding[0].code AS maritalStatus\n",
    "from df_patient\n",
    "\"\"\")\n",
    "print('Total Patient:', fm_patient.count())\n",
    "fm_patient.createOrReplaceTempView(\"patient\")\n",
    "# fm_patient.toPandas().to_parquet('_dataset/MartPatient.parquet', index=False)\n",
    "display(fm_patient.toPandas().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Encounter\n",
    "\n",
    "The Encounter is a resource that represents an interaction between a patient and healthcare provider(s) for the purpose of providing healthcare services or assessing the patient's health status.\n",
    "\n",
    "It records the full span of a hospital stay, including admission, stay and discharge. It includes details such as admission start and end time, context for the admission and patient movements within the hospital.\n",
    "\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `encounterId`: Unique identifier for the encounter.\n",
    "- `patientId`: Unique identifier for the patient.\n",
    "- `ref_encounterId`:Reference to a related encounter; can reference both future and past encounters.\n",
    "- `periodStart`: Start timestamp of the encounter period.\n",
    "- `periodEnd`:End timestamp of the encounter period.\n",
    "- `duration`: Total duration of encounter\n",
    "- `status`: Current status of the encounter (e.g., planned, in-progress, finished).\n",
    "- `encounterClass`: Classification of the encounter (e.g., inpatient, outpatient); Helps categorize the nature of the healthcare service provided.\n",
    "- `codedType`: Code representing the specific type of encounter.\n",
    "- `displayType`: Display name for the type of encounter.\n",
    "- `systemType`: System from which the type code is derived.\n",
    "- `priority`: Urgency of the encounter such as routine, urgent, or emergency; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Encounter.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicEncounter =  spark.read.schema(schema).json(\"_dataset/MimicEncounter.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicEncounterED =  spark.read.schema(schema).json(\"_dataset/MimicEncounterED.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "# df_MimicEncounterICU =  spark.read.schema(schema).json(\"_dataset/MimicEncounterICU.ndjson\") \\\n",
    "#     .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_encounter = df_MimicEncounter.union(df_MimicEncounterED) #.union(df_MimicEncounterICU)\n",
    "df_encounter.createOrReplaceTempView(\"df_encounter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_encounter = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS encounterId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(partOf.reference, \"Encounter/\", \"\") AS ref_encounterId,\n",
    "    CAST(period.start AS timestamp) periodStart,\n",
    "    CAST(period.end AS timestamp) periodEnd,\n",
    "    date_diff(day, periodStart, periodEnd) duration,\n",
    "    status AS status,\n",
    "    class.code AS encounterClass,\n",
    "    -- type[0].coding[0].code AS codedType,\n",
    "    type[0].coding[0].display AS displayType,\n",
    "    -- type[0].coding[0].system AS systemType,\n",
    "    nvl(priority.coding[0].display, 'emergency') AS priority,\n",
    "    -- Next EncounterID & Readmission Status\n",
    "    LEAD(id) OVER (PARTITION BY subject.reference ORDER BY period.start) AS nextEncounterId,\n",
    "    CASE \n",
    "        WHEN DATEDIFF(day, period.end, LEAD(period.start) OVER (PARTITION BY subject.reference ORDER BY period.start)) <= 30 THEN 'Readmission'\n",
    "        ELSE 'No Readmission'\n",
    "    END AS readmissionStatus,\n",
    "    sourceName\n",
    "FROM df_encounter\n",
    "\"\"\")\n",
    "fm_encounter.createOrReplaceTempView(\"encounter\")\n",
    "print('Total Encounter:', fm_encounter.count())\n",
    "# fm_encounter.toPandas().to_parquet('_dataset/MartEncounter.parquet', index=False)\n",
    "display(fm_encounter.toPandas().head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Condition\n",
    "\n",
    "The Condition resource in FHIR is used to record detailed information about a patient’s health state, including diagnoses, problems, or other clinical concerns. Here are the key points:\n",
    "\n",
    "- Scope and Usage: It captures conditions that have risen to a level of concern, such as diseases, health issues, or post-procedure states.\n",
    "- Clinical Context: Conditions can be recorded based on a clinician’s assessment or expressed by the patient or care team members.\n",
    "- Examples: Conditions like pregnancy, post-surgical states, or chronic illnesses can be documented. It can also include social determinants of health like unemployment or lack of transportation.\n",
    "\n",
    "Data Preprocessing Condition Code System is both ICD 9 & ICD 10, Standardize ICD-9 and ICD-10 codes to a common standard. GEMs (General Equivalence Mappings) are crosswalks between ICD-9 and ICD-10 codes. They help map codes from ICD-9-CM to ICD-10-CM and vice versa.\n",
    "\n",
    "https://www.cms.gov/Medicare/Coding/ICD10/Downloads/ICD-10_GEM_fact_sheet.pdf\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `conditionId`: The unique identifier for the condition.\n",
    "- `patientId`: The unique identifier for the patient.\n",
    "- `encounterId`: The unique identifier for the encounter.\n",
    "- `categoryCode`: Condition category.\n",
    "- `conditionCode`: The code representing the specific condition.\n",
    "- `conditionDisplay`: The display name for the condition.\n",
    "- `conditionSystem`: The system from which the condition code is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Condition.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicCondition =  spark.read.schema(schema).json(\"_dataset/MimicCondition.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicConditionED =  spark.read.schema(schema).json(\"_dataset/MimicConditionED.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_condition = df_MimicCondition.union(df_MimicConditionED)\n",
    "#  Remove Corrupt Dataset\n",
    "df_condition = df_condition.filter(df_condition[\"id\"].isNotNull())\n",
    "df_condition.createOrReplaceTempView(\"df_condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_condition = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS conditionId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(encounter.reference, \"Encounter/\", \"\") AS encounterId,\n",
    "    category[0].coding[0].code categoryCode,\n",
    "    code.coding[0].code AS conditionCode,\n",
    "    code.coding[0].display AS conditionDisplay,\n",
    "    code.coding[0].system AS conditionSystem,\n",
    "    sourceName\n",
    "FROM df_condition\n",
    "\"\"\")\n",
    "fm_condition.createOrReplaceTempView(\"condition\")\n",
    "print('Total Condition:', fm_condition.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition_size = 1_000_000\n",
    "# total_records = fm_condition.count()\n",
    "# num_partitions = (total_records // partition_size) + 1\n",
    "\n",
    "# for i in range(num_partitions):\n",
    "#     partition_df = fm_condition.limit(partition_size).offset(i * partition_size)\n",
    "#     pandas_df = partition_df.toPandas()\n",
    "#     pandas_df.to_parquet(f'_dataset/MartCondition{i}.parquet', engine='pyarrow')\n",
    "#     print(f'Condition Partition {i} saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Procedure\n",
    "\n",
    "The Procedure resource in FHIR is used to record details of current and historical procedures performed on or for a patient.\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `procedureId`: The unique identifier for the procedure.\n",
    "- `patientId`: The unique identifier for the patient.\n",
    "- `encounterId`: The unique identifier for the encounter.\n",
    "- `status`: The current status of the procedure (e.g., completed, in-progress, not-done)\n",
    "- `performedDateTime`: The date and time when the procedure was performed.\n",
    "- `procedureCode`: The code representing the specific procedure.\n",
    "- `procedureDisplay`: The display name for the procedure.\n",
    "- `procedureSystem`: The system from which the procedure code is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Procedure.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicProcedure =  spark.read.schema(schema).json(\"_dataset/MimicProcedure.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicProcedureED =  spark.read.schema(schema).json(\"_dataset/MimicProcedureED.ndjson\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "# df_MimicProcedureICU =  spark.read.schema(schema).json(\"_dataset/MimicProcedureICU.ndjson\") \\\n",
    "#     .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_procedure = df_MimicProcedure.union(df_MimicProcedureED) #.union(df_MimicProcedureICU)\n",
    "df_procedure.createOrReplaceTempView(\"df_procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_procedure = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS procedureId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(encounter.reference, \"Encounter/\", \"\") AS encounterId,\n",
    "    status AS status,\n",
    "    nvl(to_date(performedDateTime), to_date(performedPeriod.start)) AS performedDateTime,\n",
    "    code.coding[0].code AS procedureCode,\n",
    "    code.coding[0].display AS procedureDisplay,\n",
    "    code.coding[0].system AS procedureSystem,\n",
    "    sourceName\n",
    "FROM df_procedure\n",
    "\"\"\")\n",
    "fm_procedure.createOrReplaceTempView(\"procedure\")\n",
    "print('Total Procedure:', fm_procedure.count())\n",
    "# fm_procedure.toPandas().to_parquet('_dataset/procedure.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition_size = 1_000_000\n",
    "# total_records = fm_procedure.count()\n",
    "# num_partitions = (total_records // partition_size) + 1\n",
    "\n",
    "# for i in range(num_partitions):\n",
    "#     partition_df = fm_procedure.limit(partition_size).offset(i * partition_size)\n",
    "#     pandas_df = partition_df.toPandas()\n",
    "#     pandas_df.to_parquet(f'_dataset/MartProcedure{i}.parquet')\n",
    "#     print(f'Procedure Partition {i} saved successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BirthYear Distribution Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_birthYear_df = spark.sql(\"select date_format(birthDate, 'yyyy') birthYear, count(*) count  from patient group by 1 order by 1\").toPandas()\n",
    "\n",
    "fig = px.line(\n",
    "    patient_birthYear_df, \n",
    "    x='birthYear', \n",
    "    y='count', \n",
    "    title='Patient BirthYear Distribution [De-Identified]'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Birth Year',\n",
    "    yaxis_title='Number of Patients',\n",
    "    template='plotly_white'  \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender Distribution Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_count_df = fm_patient.groupBy(\"gender\").count()\n",
    "fig_patient_gender = px.pie(gender_count_df.toPandas(), values='count', names='gender', title='Gender Distribution')\n",
    "fig_patient_gender.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gender & Marital Status Grouped Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_marital_status_count_df = fm_patient.groupBy(\"gender\", \"maritalStatus\").count()\n",
    "fig_patient_gender_marital_status = px.bar(\n",
    "    gender_marital_status_count_df.toPandas(), \n",
    "    x='maritalStatus', \n",
    "    y='count', \n",
    "    color='gender', \n",
    "    title='Count by Gender and Marital Status', \n",
    "    barmode='group'\n",
    ")\n",
    "fig_patient_gender_marital_status.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Encounter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Year Distribution Line Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_year_df = spark.sql(\"select date_format(periodStart, 'yyyy') encounterYear, count(*) count  from encounter group by 1 order by 1\").toPandas()\n",
    "\n",
    "fig = px.line(\n",
    "    encounter_year_df, \n",
    "    x='encounterYear', \n",
    "    y='count', \n",
    "    title='Encounter Period Year Distribution [De-Identified]'\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Number of Encounters',\n",
    "    template='plotly_white'  \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Source Distribution Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(fm_encounter.groupBy(\"sourceName\").count().toPandas(), values='count', names='sourceName', title='Source Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Class Distribution Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(fm_encounter.groupBy(\"encounterClass\").count().toPandas(), values='count', names='encounterClass', title='Encounter Class Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Priority Distribution Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(fm_encounter.groupBy(\"priority\").count().toPandas(), values='count', names='priority', title='Encounter Priority Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Class by Priority Grouped Bar Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = fm_encounter.groupBy(\"encounterClass\", \"priority\").count().toPandas()\n",
    "\n",
    "fig = px.bar(\n",
    "    grouped_data, \n",
    "    x='priority', \n",
    "    y='count', \n",
    "    color='encounterClass', \n",
    "    title='Encounter Class by Priority', \n",
    "    labels={'count': 'Count'},\n",
    "    barmode='group'\n",
    ")\n",
    "\n",
    "fig.update_layout(xaxis_title='Priority',yaxis_title='Count',legend_title='Encounter Class')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Patient Readmission Pie Chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(fm_encounter.groupBy(\"readmissionStatus\").count().toPandas(), values='count', names='readmissionStatus', title='Patient Readmission').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Over the Years by ReadmissionStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "readmission_df = fm_encounter.groupBy(\n",
    "    \"readmissionStatus\", F.date_format(\"periodStart\", \"yyyy\").alias(\"year\")\n",
    ").count().orderBy(\"year\")\n",
    "\n",
    "fig = px.line(readmission_df.toPandas(), \n",
    "              x='year', \n",
    "              y='count', \n",
    "              color='readmissionStatus',  # Differentiate lines by readmissionStatus\n",
    "              title='Encounter Over the Years by ReadmissionStatus', \n",
    "              labels={'year': 'Year', 'count': 'Number of Encounters'})\n",
    "\n",
    "fig.update_layout(\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Number of Encounters',\n",
    "    legend_title='Readmission Status',\n",
    "    template='plotly_white'  \n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Readmission Encounters: Priority Transitions Sankey Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_encounter_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    e1.priority priority1,\n",
    "    e2.priority priority2,\n",
    "    count(*) count\n",
    "FROM encounter e1\n",
    "JOIN encounter e2\n",
    "ON e1.nextEncounterId = e2.encounterID\n",
    "WHERE e1.readmissionStatus == 'Readmission'\n",
    "GROUP BY all\n",
    "ORDER by 3 desc\n",
    "\"\"\")\n",
    "\n",
    "priority_encounter_pd = priority_encounter_df.toPandas()\n",
    "all_priorities = list(set(priority_encounter_pd['priority1'].tolist() + \n",
    "                          priority_encounter_pd['priority2'].tolist()))\n",
    "node_map = {priority: idx for idx, priority in enumerate(all_priorities)}\n",
    "priority_encounter_pd['source'] = priority_encounter_pd['priority1'].map(node_map)\n",
    "priority_encounter_pd['target'] = priority_encounter_pd['priority2'].map(node_map)\n",
    "\n",
    "table_trace = go.Table(\n",
    "    header=dict(values=[\"Priority 1\", \"Priority 2\", \"Count\"], fill_color='lightgrey', align='center'),\n",
    "    cells=dict(values=[priority_encounter_pd['priority1'], priority_encounter_pd['priority2'], priority_encounter_pd['count']],\n",
    "               fill_color='white', align='center')\n",
    ")\n",
    "\n",
    "sankey_trace = go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,  # Padding between nodes\n",
    "        thickness=20,  # Node thickness\n",
    "        line=dict(color=\"black\", width=0.5),  # Node border settings\n",
    "        label=all_priorities  # Node labels\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=priority_encounter_pd['source'],  # Source nodes (indices)\n",
    "        target=priority_encounter_pd['target'],  # Target nodes (indices)\n",
    "        value=priority_encounter_pd['count']     # Flow values (counts)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2, \n",
    "    column_widths=[0.7, 0.3],  # Adjust column widths (30% table, 70% Sankey)\n",
    "    specs=[[{\"type\": \"table\"}, {\"type\": \"sankey\"}]],  # Specify types for each subplot\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(sankey_trace, row=1, col=1)\n",
    "fig.add_trace(table_trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Readmission Encounters: Priority Transitions Sankey Diagram and Table\",\n",
    "    font_size=12,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encounter Count by Source and Duration Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql --l 100\n",
    "SELECT \n",
    "    sourceName, \n",
    "    CASE \n",
    "        WHEN duration BETWEEN 0 AND 5 THEN '0 to 5 days'\n",
    "        WHEN duration BETWEEN 6 AND 10 THEN '6 to 10 days'\n",
    "        WHEN duration BETWEEN 11 AND 20 THEN '11 to 20 days'\n",
    "        WHEN duration BETWEEN 21 AND 30 THEN '21 to 30 days'\n",
    "        WHEN duration BETWEEN 31 AND 50 THEN '31 to 50 days'\n",
    "        WHEN duration BETWEEN 51 AND 100 THEN '51 to 100 days'\n",
    "        WHEN duration BETWEEN 101 AND 150 THEN '101 to 150 days'\n",
    "        WHEN duration BETWEEN 151 AND 200 THEN '151 to 200 days'\n",
    "        WHEN duration BETWEEN 201 AND 250 THEN '201 to 250 days'\n",
    "        WHEN duration BETWEEN 251 AND 300 THEN '251 to 300 days'\n",
    "        ELSE 'More than 300 days'\n",
    "    END AS duration_group,\n",
    "    COUNT(*) AS encounter_count\n",
    "FROM encounter\n",
    "GROUP BY sourceName, duration_group\n",
    "ORDER BY sourceName, double(split_part(duration_group, ' ', 1));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_duration_df = spark.sql(\"\"\"SELECT \n",
    "    sourceName,\n",
    "    duration AS duration_group,\n",
    "    COUNT(*) AS encounter_count\n",
    "FROM encounter\n",
    "where duration BETWEEN 1 AND 30\n",
    "GROUP BY sourceName, duration_group\n",
    "ORDER BY sourceName, duration_group;\"\"\").toPandas()\n",
    "\n",
    "px.line(encounter_duration_df, x='duration_group', y='encounter_count', color='sourceName',\n",
    "              markers=True, title=\"Encounter Count by Source and Duration Group\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from condition limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "condition_system_df = spark.sql(\"select split_part(conditionSystem, '/', -1) conditionSystem, count(*) count from condition group by 1\").toPandas()\n",
    "px.pie(condition_system_df, values='count', names='conditionSystem', title='Condition System Count Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `Condition System Count Distribution` chart, the dataset incorporates both ICD-9 and ICD-10 coding systems in condition table.\n",
    "\n",
    "The difference between these two coding systems can introduce challenges which may affect the model performance and interpretability.\n",
    "\n",
    "1. Inconsistent Coding:\n",
    "    - ICD-10 has more granularity in specifying types of diseases, injury location, and severity.\n",
    "    - ICD-9 has fewer and less specific codes than ICD-10, which means that a single ICD-9 code could map to multiple ICD-10 codes.\n",
    "    - This inconsistency can create noisy features in your dataset if the same condition is coded differently depending on the coding system used. This could confuse your model, leading to reduced predictive accuracy.\n",
    "2. Feature Engineering Complexity:\n",
    "    - ICD-9 and ICD-10 are structured differently, both in terms of the number of codes and their specificity.\n",
    "    - This complicates the creation of features related to diagnosis categories or comorbidities.\n",
    "    - The features might not capture the full clinical picture, potentially leading to underfitting or overfitting\n",
    "3. Data Heterogeneity:\n",
    "    - Introduction of temporal bias into the model\n",
    "    - This could cause the model to overestimate or underestimate readmission risks if it correlates newer coding systems with better or worse outcomes.\n",
    "4. Model Interpretability:\n",
    "    - Mixed coding systems make model interpretation harder, especially if using interpretable models like decision trees or logistic regression.\n",
    "    - This may end up with features that are not comparable between ICD-9 and ICD-10, complicating efforts to explain your model's predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select conditionCode, conditionDisplay, count(*) count from condition\n",
    "where conditionSystem like '%mimic-diagnosis-icd10'\n",
    "group by all order by 3 desc \n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select conditionCode, conditionDisplay, count(*) count from condition\n",
    "where conditionSystem like '%mimic-diagnosis-icd9'\n",
    "group by all order by 3 desc \n",
    "limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On comparing top condition for both coding systems, there are few notable common types between them.\n",
    "\n",
    "For Example:\n",
    "\n",
    "- ICD 9 Code 4019 `Unspecified essential hypertension` is similar to ICD 10 Code I10 `Essential (primary) hypertension`.\n",
    "- ICD 9 Code V1582 `Personal history of tobacco use` is similar to ICD 10 Code Z87891 `Personal history of nicotine dependence`.\n",
    "- ICD 9 Code 2724 `Other and unspecified hyperlipidemia\t` is similar to ICD 10 Code E785 `Hyperlipidemia, unspecified`.\n",
    "\n",
    "The dataset must be standardized in order to gain better accuracy of predictive model.\n",
    "\n",
    "One solution is to map ICD-9 codes to ICD-10 equivalents using tools like the General Equivalence Mappings (GEMs). This allows you to convert ICD-9 codes to ICD-10 to standardize the datase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from procedure limit 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "procedure_system_df = spark.sql(\"select split_part(procedureSystem, '/', -1) procedureSystem, count(*) count from procedure group by 1\").toPandas()\n",
    "px.pie(procedure_system_df, values='count', names='procedureSystem', title='Procedure System Count Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to Condition Domain, Procedure Domain has an extra coding system `http://snomed.info/sct`\n",
    "- Issue with ICD-9 & ICD-10 can easily be resolved by converting them.\n",
    "- New issue arose here with SCT coding system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select distinct procedureCode, procedureDisplay from procedure\n",
    "where procedureSystem = 'http://snomed.info/sct'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coding System `http://snomed.info/sct` only has two procedure code (386478007 & 410188000).\n",
    "- These procedures are quite common and performed for nearly every patient in an emergency or hospital setting.\n",
    "- They are generic procedures and may not help differentiate between patients who are at high risk of readmission versus those who are not due to its low variability.\n",
    "- Including procedures like these may add noise to the model and clutter the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Condition Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardarize Coding System\n",
    "\n",
    "There are two coding system in condition domain: ICD-9 & ICD-10.\n",
    "\n",
    "To facilitate transition, CMS and CDC developed GEMs, which act as crosswalks, translating ICD-9 codes to their ICD-10 equivalents.\n",
    "\n",
    "Learn more about this crosswalk on [ICD-9-CM to and from ICD-10-CM and ICD-10-PCS Crosswalk or General Equivalence Mappings](https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT conditionSystem, count(*) FROM condition group by conditionSystem order by 1 desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICD-10 Code Structure\n",
    "- Characters 1:3 = Indicate the category of the diagnosis\n",
    "- Characters 4:6 = Indicate etiology, anatomic site, severity or other clinical detail\n",
    "- Character 7 = Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"_dataset/icd9toicd10cmgem.csv\", header = True)\n",
    "df.createOrReplaceTempView(\"GemMapping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "standardize_condition_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM condition \n",
    "    WHERE condition.conditionSystem LIKE '%icd10'\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        condition.conditionId,\n",
    "        condition.patientId,\n",
    "        condition.encounterId,\n",
    "        condition.categoryCode,\n",
    "        GemMapping.icd10cm AS conditionCode,\n",
    "        CAST(NULL AS STRING) conditionDisplay,\n",
    "        CAST(NULL AS STRING) AS conditionSystem,\n",
    "        condition.sourceName\n",
    "    FROM condition \n",
    "    JOIN GemMapping\n",
    "    ON condition.conditionCode = GemMapping.icd9cm\n",
    "    WHERE condition.conditionSystem LIKE '%icd9'\n",
    "\"\"\")\n",
    "\n",
    "standardize_condition_df.createOrReplaceTempView(\"ICD10Condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "source": [
    "#### Pivot Condition Table based on Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot_df = spark.sql(\"\"\"\n",
    "    WITH RankedConditions AS (\n",
    "        SELECT \n",
    "            patientId,\n",
    "            encounterId,\n",
    "            conditionCode,\n",
    "            COUNT(*) AS conditionCount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY encounterId ORDER BY COUNT(*) DESC) AS rank\n",
    "        FROM ICD10Condition\n",
    "        GROUP BY patientId, encounterId, conditionCode\n",
    "    )\n",
    "    SELECT\n",
    "        patientId,\n",
    "        encounterId,\n",
    "        MAX(CASE WHEN rank = 1 THEN conditionCode END) AS condition_1,\n",
    "        MAX(CASE WHEN rank = 2 THEN conditionCode END) AS condition_2,\n",
    "        MAX(CASE WHEN rank = 3 THEN conditionCode END) AS condition_3\n",
    "    FROM RankedConditions\n",
    "    GROUP BY patientId, encounterId\n",
    "\"\"\")\n",
    "# condition_pivot_df.createOrReplaceTempView('condition_pivot_df')\n",
    "# .toPandas()\n",
    "# condition_pivot_df.to_parquet('_dataset/PivotCondition.parquet', engine='pyarrow',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select * from condition_pivot_df limit 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Pivot Condition Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%dql\n",
    "SELECT count(*) FROM read_parquet(\"_dataset/MartCondition2.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot_df = spark.sql(\"\"\"\n",
    "    WITH RankedConditions AS (\n",
    "        SELECT \n",
    "            patientId,\n",
    "            encounterId,\n",
    "            conditionDisplay,\n",
    "            COUNT(*) AS conditionCount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY encounterId ORDER BY COUNT(*) DESC) AS rank\n",
    "        FROM condition\n",
    "        GROUP BY patientId, encounterId, conditionDisplay\n",
    "    )\n",
    "    SELECT\n",
    "        patientId,\n",
    "        encounterId,\n",
    "        MAX(CASE WHEN rank = 1 THEN conditionDisplay END) AS condition_1,\n",
    "        -- MAX(CASE WHEN rank = 1 THEN conditionCount END) AS condition_count_1,\n",
    "        MAX(CASE WHEN rank = 2 THEN conditionDisplay END) AS condition_2,\n",
    "        -- MAX(CASE WHEN rank = 2 THEN conditionCount END) AS condition_count_2,\n",
    "        MAX(CASE WHEN rank = 3 THEN conditionDisplay END) AS condition_3\n",
    "        -- MAX(CASE WHEN rank = 3 THEN conditionCount END) AS condition_count_3\n",
    "    FROM RankedConditions\n",
    "    GROUP BY patientId, encounterId\n",
    "\"\"\").toPandas()\n",
    "# condition_pivot_df.to_parquet('_dataset/PivotCondition.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Pivot Procedure Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure_pivot_df = spark.sql(\"\"\"\n",
    "    WITH RankedProcedures AS (\n",
    "        SELECT \n",
    "            patientId,\n",
    "            encounterId,\n",
    "            procedureDisplay,\n",
    "            COUNT(*) AS procedureCount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY encounterId ORDER BY COUNT(*) DESC) AS rank\n",
    "        FROM procedure\n",
    "        GROUP BY patientId, encounterId, procedureDisplay\n",
    "    )\n",
    "    SELECT\n",
    "        patientId,\n",
    "        encounterId,\n",
    "        MAX(CASE WHEN rank = 1 THEN procedureDisplay END) AS procedure_1,\n",
    "        -- MAX(CASE WHEN rank = 1 THEN procedureCount END) AS procedure_count_1,\n",
    "        MAX(CASE WHEN rank = 2 THEN procedureDisplay END) AS procedure_2,\n",
    "        -- MAX(CASE WHEN rank = 2 THEN procedureCount END) AS procedure_count_2,\n",
    "        MAX(CASE WHEN rank = 3 THEN procedureDisplay END) AS procedure_3\n",
    "        -- MAX(CASE WHEN rank = 3 THEN procedureCount END) AS procedure_count_3\n",
    "    FROM RankedProcedures\n",
    "    GROUP BY patientId, encounterId\n",
    "\"\"\").toPandas()\n",
    "# procedure_pivot_df.to_parquet('_dataset/PivotProcedure.parquet', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Consolidate Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "COPY (\n",
    "    SELECT\n",
    "        encounter.encounterId,\n",
    "        encounter.patientId,\n",
    "        patient.gender,\n",
    "        CASE \n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 18 AND 29 THEN 'young adults'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 30 AND 39 THEN 'young adulthood'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 40 AND 49 THEN 'early-middle age'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 50 AND 59 THEN 'late-middle age'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 60 AND 69 THEN 'mid-old age'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 70 AND 79 THEN 'senior-old age'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 80 AND 89 THEN 'very senior-old'\n",
    "            WHEN date_diff('year', patient.birthDate,encounter.periodStart) BETWEEN 90 AND 115 THEN 'centenarians'\n",
    "            ELSE 'other age groups'\n",
    "        END AS ageGroup,\n",
    "        patient.maritalStatus,\n",
    "        encounter.duration encounterDuration,\n",
    "        encounter.status encounterStatus,\n",
    "        encounter.encounterClass,\n",
    "        encounter.displayType encounterType,\n",
    "        encounter.priority,\n",
    "        -- Conditions\n",
    "        condition.condition_1,\n",
    "        condition.condition_2,\n",
    "        condition.condition_3,\n",
    "        -- Procedures\n",
    "        procedure.procedure_1,\n",
    "        procedure.procedure_2,\n",
    "        procedure.procedure_3,\n",
    "        encounter.readmissionStatus,\n",
    "        encounter.sourceName\n",
    "    FROM read_parquet(\"_dataset/encounter.parquet\") encounter\n",
    "    LEFT JOIN read_parquet(\"_dataset/patient.parquet\") patient\n",
    "        ON encounter.patientId = patient.patientID\n",
    "    LEFT JOIN read_parquet(\"_dataset/PivotCondition.parquet\") condition\n",
    "        ON encounter.encounterId = condition.encounterId\n",
    "        AND encounter.patientID = condition.patientID\n",
    "    LEFT JOIN read_parquet(\"_dataset/PivotProcedure.parquet\") procedure\n",
    "        ON encounter.encounterId = procedure.encounterId\n",
    "        AND encounter.patientID = procedure.patientID\n",
    ")\n",
    "TO '_dataset/ConsolidatedEncounter.parquet'\n",
    "(FORMAT 'parquet');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training\n",
    "\n",
    "Data Source: `_dataset/ConsolidatedEncounter.parquet`\n",
    "\n",
    "- `encounterId`: Unique Identifier for the Encounter\n",
    "- `patientId`: Patient Unique Identifier\n",
    "- `gender`: Patient Gender\n",
    "- `ageGroup`: Patient Age Group\n",
    "- `maritalStatus`: Patient Marital Status\n",
    "- `encounterDuration`: Total Encounter Duration in days\n",
    "- `encounterStatus`: Current status of the encounter (e.g., planned, in-progress, finished).\n",
    "- `encounterClass`: Classification of the encounter (e.g., inpatient, outpatient); Helps categorize the nature of the healthcare service provided.\n",
    "- `encounterType`: Specific type of Encounter\n",
    "- `priority`: Urgency of the encounter such as routine, urgent, or emergency; \n",
    "- `condition_1`: Primary Condition\n",
    "- `condition_2`: Secondary Condition\n",
    "- `condition_3`: Other Condition\n",
    "- `procedure_1`: Primary Procedure\n",
    "- `procedure_2`: Secondary Procedure\n",
    "- `procedure_3`: Other Procedure\n",
    "- `readmissionStatus`: Is the Encounter Readmitted in the Future\n",
    "- `sourceName`: Source of the Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"_dataset/ConsolidatedEncounter.parquet\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['gender'] = df['gender'].astype('category')\n",
    "df['ageGroup'] = df['ageGroup'].astype('category')\n",
    "df['maritalStatus'] = df['maritalStatus'].astype('category')\n",
    "\n",
    "df['encounterStatus'] = df['encounterStatus'].astype('category')\n",
    "df['encounterClass'] = df['encounterClass'].astype('category')\n",
    "df['encounterType'] = df['encounterType'].astype('category')\n",
    "df['priority'] = df['priority'].astype('category')\n",
    "df['encounterStatus'] = df['encounterStatus'].astype('category')\n",
    "\n",
    "\n",
    "df['condition_1'] = df['condition_1'].astype('category')\n",
    "df['condition_2'] = df['condition_2'].astype('category')\n",
    "df['condition_3'] = df['condition_3'].astype('category')\n",
    "\n",
    "df['procedure_1'] = df['procedure_1'].astype('category')\n",
    "df['procedure_2'] = df['procedure_2'].astype('category')\n",
    "df['procedure_3'] = df['procedure_3'].astype('category')\n",
    "\n",
    "df['readmissionStatus'] = df['readmissionStatus'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the unique values in the dataset\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null value\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the duplicate values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = df['readmissionStatus'].map({'Readmission': 1, 'No Readmission': 0})\n",
    "\n",
    "x = df['ageGroup'].cat.codes\n",
    "y = df['gender'].cat.codes\n",
    "z = df['maritalStatus'].cat.codes\n",
    "\n",
    "c = es\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "img = ax.scatter(x, y, z, c=c, cmap='coolwarm', marker='o')\n",
    "fig.colorbar(img)\n",
    "\n",
    "ax.set_xlabel('Age Group')\n",
    "ax.set_ylabel('Gender')\n",
    "ax.set_zlabel('Marital Status')\n",
    "\n",
    "plt.title('Count of Readmitted Patients for Patient Demographics')\n",
    "plt.legend(*img.legend_elements(), title='Readmitted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. LabelEncoder \n",
    "\n",
    "Convert categorical string labels (or values) into numeric labels. This is often required for machine learning algorithms that expect numerical input rather than categorical text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the LabelEncoder\n",
    "le = LabelEncoder()\n",
    "# Apply the LabelEncoder to the categorical columns\n",
    "df['gender'] = le.fit_transform(df['gender'])\n",
    "df['ageGroup'] = le.fit_transform(df['ageGroup'])\n",
    "df['maritalStatus'] = le.fit_transform(df['maritalStatus'])\n",
    "df['encounterStatus'] = le.fit_transform(df['encounterStatus'])\n",
    "df['encounterClass'] = le.fit_transform(df['encounterClass'])\n",
    "df['encounterType'] = le.fit_transform(df['encounterType'])\n",
    "df['encounterStatus'] = le.fit_transform(df['encounterStatus'])\n",
    "df['priority'] = le.fit_transform(df['priority'])\n",
    "df['condition_1'] = le.fit_transform(df['condition_1'])\n",
    "df['condition_2'] = le.fit_transform(df['condition_2'])\n",
    "df['condition_3'] = le.fit_transform(df['condition_3'])\n",
    "df['procedure_1'] = le.fit_transform(df['procedure_1'])\n",
    "df['procedure_2'] = le.fit_transform(df['procedure_2'])\n",
    "df['procedure_3'] = le.fit_transform(df['procedure_3'])\n",
    "df['readmissionStatus'] = le.fit_transform(df['readmissionStatus'])\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='encounterId', inplace=True)\n",
    "df.drop(columns='patientId', inplace=True)\n",
    "df.drop(columns='sourceName', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show correlation of numerical columns\n",
    "df_numeric = df.select_dtypes(include=['number'])\n",
    "\n",
    "# Calculate correlation matrix between selected columns\n",
    "correlation_matrix = df_numeric.corr()\n",
    "\n",
    "# Visualizing correlation between variables\n",
    "plt.figure(figsize=(16,12))\n",
    "sns.heatmap(correlation_matrix, cmap='YlGnBu', fmt='.2g', annot=True)\n",
    "\n",
    "plt.title('Heatmap showing correlation amongst variables',y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Split Train and Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into two DataFrames: X (features) and y (target variable)\n",
    "X = df.drop(columns=['readmissionStatus'],axis=1)  # Specify at least one column as a feature\n",
    "y = df[\"readmissionStatus\"]  # Specify one column as the target variable\n",
    "\n",
    "# Split the data into train and test subsets\n",
    "# You can adjust the test size and random state\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.275, random_state=123\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters: these will need to be tuned to prevent overfitting and underfitting\n",
    "params = {\n",
    "    \"n_estimators\": 97,  # Number of trees in the forest\n",
    "    \"max_depth\": 10,  # Max depth of the tree\n",
    "    \"min_samples_split\": 3,  # Min number of samples required to split a node\n",
    "    \"min_samples_leaf\": 1,  # Min number of samples required at a leaf node\n",
    "    \"ccp_alpha\": 0,  # Cost complexity parameter for pruning\n",
    "    \"random_state\": 123,\n",
    "}\n",
    "#{'n_estimators': 97, 'min_samples_split': 3, 'min_samples_leaf': 1, 'max_depth': 10, 'ccp_alpha': 0}\n",
    "\n",
    "# Create a RandomForestRegressor object with the parameters above\n",
    "rf = RandomForestClassifier(**params)\n",
    "\n",
    "# Train the random forest on the train set\n",
    "rf = rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the outcomes on the test set\n",
    "y_pred = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "print('Training Set:', rf.score(X_train,y_train))\n",
    "print('Test Set:', rf.score(X_test,y_test))\n",
    "\n",
    "# Calculate the accuracy, precision, and recall scores\n",
    "print(\"Accuracy:\", \"{:.2f}%\".format(accuracy_score(y_test, y_pred)*100))\n",
    "print(\"Precision:\", \"{:.2f}%\".format(precision_score(y_test, y_pred)*100))\n",
    "print(\"Recall:\", \"{:.2f}%\".format(recall_score(y_test, y_pred)*100))\n",
    "print(\"Area Under the Curve:\", \"{:.2f}%\".format(roc_auc_score(y_test, y_pred)*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
