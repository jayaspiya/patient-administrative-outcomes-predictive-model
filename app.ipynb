{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Administrative Outcomes Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Setup Development Environment \n",
    "\n",
    "- Install Poetry to simplify dependency management. \n",
    "    - `pip install poetry` \n",
    "- Setup Poetery Project \n",
    "    - `poetry init` \n",
    "- Create a virtual envrionment \n",
    "    - `python -m venv .venv` \n",
    "- Add dependencies \n",
    "    - `poetry add pyspark ipykernel pandas matplotlib ploty seaborn`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Data Integration\n",
    "\n",
    "<!-- Data integration with Fast Healthcare Interoperability Resources (FHIR) resources involves unifying healthcare data from multiple systems to enable seamless data exchange and interoperability. This approach enhances data consistency, accessibility, and streamlines access to critical healthcare information across applications and organizations.\n",
    "\n",
    "**Data Source: MIMIC-IV on FHIR**\n",
    "\n",
    "The project uses MIMIC-IV on FHIR as its primary data source. This dataset provides a rich, de-identified healthcare database mapped to the FHIR (Fast Healthcare Interoperability Resources) standard to support interoperability and advanced analytics in healthcare research.\n",
    "\n",
    "MIMIC-IV on FHIR is a project by PhysioNet designed to translate the structure and content of MIMIC-IV into the FHIR standard. MIMIC-IV is a relational database containing real hospital stay data for patients admitted to a tertiary academic medical center in Boston, MA, USA. It includes comprehensive clinical and administrative information recorded between 2008 and 2019.\n",
    "\n",
    "By leveraging MIMIC-IV on FHIR, this project aims to enhance data interoperability and enable the development of predictive models and analytical insights aligned with modern healthcare standards and regulatory compliance.\n",
    "\n",
    "**Fast Healthcare Interoperability Resources (FHIR)**\n",
    "\n",
    "FHIR is a specification for exchanging health care data across different systems and platforms. The specification builds on and adapts modern, widely used RESTful practices to enable the provision of integrated healthcare across a wide range of teams and organizations. FHIR is based on \"Resources\" which are the common building blocks for all exchanges.\n",
    "\n",
    "Key FHIR Resources for Integration:\n",
    "\n",
    "- Patient: Stores demographic information such as name, age, and contact details.\n",
    "- Encounter: Captures details of patient visits and interactions with healthcare providers.\n",
    "- Condition: Represents diagnoses and health conditions affecting the patient.\n",
    "- Procedure: Records medical procedures and interventions performed.\n",
    "- Observation: Contains lab results, vital signs, and other clinical observations.\n",
    "\n",
    "**Medallion Architecture**\n",
    "\n",
    "The project follows a Medallion Architecture framework, which organizes data into multiple layers to ensure structured data refinement, improved data quality, and optimized analytics. This approach enhances data governance and enables efficient processing for downstream use cases.\n",
    "\n",
    "The architecture comprises three key layers:\n",
    "1. Bronze Layer (Raw Data): The Bronze layer stores raw, ingested data from various source systems without transformations. It retains all original fields and formats, providing a comprehensive source of truth for historical and traceability purposes.\n",
    "\n",
    "2. Silver Layer (Filtered, Cleaned, Augmented): The Silver layer refines and transforms the raw data, selecting only the necessary fields from relevant FHIR resources (e.g., Patient, Encounter, Condition, Procedure). This layer provides clean, structured, and domain-specific datasets for targeted analysis and processing.\n",
    "\n",
    "3. Gold Layer (Business-Level Aggregrates): The Gold layer aggregates data into a unified, optimized format for advanced analytics and reporting. In this project, it focuses on a Consolidated Encounter Table, combining encounter data with relevant patient demographics, conditions, and other clinical information to support predictive modeling and operational insights.\n",
    "\n",
    "This layered architecture promotes scalability, data quality, and efficient data retrieval, aligning with best practices for healthcare data management and analytics.\n",
    "\n",
    "> **References**\n",
    "> - [MIMIC Implementatoin Guide | KinD Lab](https://mimic.mit.edu/fhir/)\n",
    "> - [FHIR Documentation | HL7FHIR](https://www.hl7.org/fhir/)\n",
    "> - [Medallion Architecture | Databricks](https://www.databricks.com/glossary/medallion-architecture) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FhirDataApplication\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.local\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.local.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.local.warehouse\", \"dataset/iceberg\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bronze Layer\n",
    "\n",
    "<!-- The Bronze Layer serves as the foundation of the data architecture, capturing raw, untransformed data from various source systems. It retains all original fields and formats, providing a comprehensive source of truth for historical and traceability purposes. This layer plays a crucial role in maintaining data integrity, enabling auditability, and supporting complex analytical workflows in later stages.\n",
    "\n",
    "In the project, `bronze.resource` serves as a storage for raw data that has been ingested from various source files. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/bronze.resource.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESOURCE_FILES = [\"MimicPatient.ndjson.gz\", \"MimicCondition.ndjson.gz\", \"MimicConditionED.ndjson.gz\", \"MimicEncounter.ndjson.gz\", \"MimicEncounterED.ndjson.gz\", \"MimicEncounterICU.ndjson.gz\"] \n",
    "for resource_file in RESOURCE_FILES: \n",
    "    resource_df = spark.read.text(f\"dataset/{resource_file}\") \n",
    "    resource_df = resource_df.withColumn(\"sourceFile\", F.lit(resource_file)) \n",
    "    resource_df.write.format(\"iceberg\").mode(\"append\").save(\"local.bronze.resource\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Volume in Each File\")\n",
    "data_volume_df = spark.sql(\"select sourceFile, count(*) cnt from  local.bronze.resource group by 1\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 8))\n",
    "bars = plt.bar(data_volume_df['sourceFile'], data_volume_df['cnt'])\n",
    "\n",
    "plt.xlabel('Categories')\n",
    "plt.ylabel('Values')\n",
    "plt.title('Bar Chart with Full Number Labels')\n",
    "\n",
    "for bar in bars:\n",
    "    plt.text(\n",
    "        bar.get_x() + bar.get_width() / 2,  # X position\n",
    "        bar.get_height(),                   # Y position\n",
    "        f'{bar.get_height():,}',            # Format with commas\n",
    "        ha='center', va='bottom', fontsize=10  # Center alignment, font size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Silver Layer\n",
    "\n",
    "<!-- The Silver Table represents the refined data layer in the Medallion Architecture, where raw data from the Bronze Layer is transformed and cleansed. It focuses on structuring the data and selecting only the necessary fields from relevant sources, ensuring that it is ready for more detailed analysis and reporting.\n",
    "\n",
    "The Silver Tables in use include Patient, Encounter, and Condition. The Patient table stores essential demographic information, while the Encounter table tracks patient visits and interactions with healthcare providers. The Condition table holds information about patient diagnoses and health conditions.\n",
    "\n",
    "The resources are stored while adhering to Safe Harbor provisions to ensure patient privacy and compliance with data protection regulations.\n",
    "\n",
    "**Safe Harbor Provisions**\n",
    "\n",
    "Safe Harbor provisions are a set of guidelines that allow organizations to de-identify personal data in a way that ensures compliance with privacy regulations, particularly in the context of healthcare and data protection laws like HIPAA. Under these provisions, data can be stripped of specific identifiers, such as names, addresses, and Social Security numbers, making it no longer considered protected health information (PHI). Safe Harbor provisions offer a balance between privacy protection and the need for data availability, enabling the use of anonymized data while minimizing the risk of re-identification.\n",
    "\n",
    "\n",
    "> **References**\n",
    "> - [Guidance Regarding Methods for De-identification of PHI](https://www.hhs.gov/hipaa/for-professionals/special-topics/de-identification/index.html) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Bronze Table\n",
    "raw_df = spark.read.format(\"iceberg\").load(\"local.bronze.resource\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patient Mart\n",
    "\n",
    "<!-- The Patient Mart stores essential demographic information for each patient, providing a foundational set of details necessary for understanding patient profiles and is used in various healthcare processes, from clinical interactions to research and analysis.\n",
    "\n",
    "| Column Name   | Description                                   |\n",
    "|---------------|-----------------------------------------------|\n",
    "| patientID     | A unique identifier assigned to each patient. |\n",
    "| gender        | The patient's gender (e.g., male or female).  |\n",
    "| birthDate     | The patient's date of birth.                  |\n",
    "| maritalStatus | The marital status of the patient.            | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.patient.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_df = raw_df.filter(raw_df.sourceFile.isin(\"MimicPatient.ndjson.gz\"))\n",
    "with open(\"schema/Patient.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema = T.StructType.fromJson(schema_read)\n",
    "patient_df = patient_df.withColumn(\"parsed_json\", F.from_json(patient_df[\"value\"], schema))\n",
    "patient_df = patient_df.select(\"parsed_json.*\", \"sourceFile\")\n",
    "patient_df.createOrReplaceTempView(\"patient_df\")\n",
    "\n",
    "with open(\"sql_queries/silver.patient.load.sql\") as f:\n",
    "    fm_patient = spark.sql(f.read())\n",
    "\n",
    "fm_patient.show(5, truncate=False)\n",
    "fm_patient.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.silver.patient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encounter Mart\n",
    "\n",
    "<!-- The Encounter resource represents an interaction between a patient and healthcare provider(s) for the purpose of delivering healthcare services or assessing the patient’s health status.\n",
    "\n",
    "It captures the full span of a hospital stay, from admission to discharge, and includes important details such as the start and end time of the admission, the context surrounding the admission, and patient movements within the healthcare facility.\n",
    "\n",
    "| Column            | Description                                                                                                                          |\n",
    "|-------------------|--------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| encounterId       | A unique identifier for the encounter.                                                                                               |\n",
    "| patientId         | A unique identifier for the patient involved in the encounter.                                                                       |\n",
    "| ref_encounterId   | A reference to a related encounter, which may relate to both future and past encounters.                                             |\n",
    "| periodStart       | The start timestamp marking the beginning of the encounter period.                                                                   |\n",
    "| periodEnd         | The end timestamp marking the conclusion of the encounter period.                                                                    |\n",
    "| duration          | The total duration of the encounter, typically in hours or days.                                                                     |\n",
    "| status            | The current status of the encounter (e.g., planned, in-progress, finished).                                                          |\n",
    "| encounterClass    | The classification of the encounter, such as inpatient or outpatient, helping to categorize the type of healthcare service provided. |\n",
    "| codedType         | A code representing the specific type of encounter, aiding in standardized classification.                                           |\n",
    "| displayType       | A human-readable display name for the type of encounter.                                                                             |\n",
    "| systemType        | The system from which the encounter type code is derived.                                                                            |\n",
    "| priority          | The urgency level of the encounter, which may include options like routine, urgent, or emergency.                                    |\n",
    "| nextEncounterId   | A reference to next encounter                                                                                                        |\n",
    "| readmissionStatus | Future readmission indicator                                                                                                         | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.encounter.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_df = raw_df.filter(raw_df.sourceFile.isin(\"MimicEncounter.ndjson.gz\", \"MimicEncounterED.ndjson.gz\")) \n",
    "with open(\"schema/Encounter.json\") as f: \n",
    "    schema_read = json.loads(f.read()) \n",
    "schema = T.StructType.fromJson(schema_read) \n",
    "encounter_df = encounter_df.withColumn(\"parsed_json\", F.from_json(encounter_df[\"value\"], schema)) \n",
    "encounter_df = encounter_df.select(\"parsed_json.*\", \"sourceFile\") \n",
    "encounter_df.createOrReplaceTempView(\"encounter_df\") \n",
    "\n",
    "with open(\"sql_queries/silver.encounter.load.sql\") as f:\n",
    "    fm_encounter = spark.sql(f.read())\n",
    "\n",
    "fm_encounter.show(5, truncate=False) \n",
    "fm_encounter.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.silver.encounter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition Mart\n",
    "\n",
    "<!-- The Condition resource in FHIR is used to document a patient’s health state, including diagnoses, clinical problems, or other health concerns.\n",
    "\n",
    "It captures conditions that are significant enough to warrant clinical focus, including diseases, health problems, or the state following a medical procedure.\n",
    "\n",
    "This resource plays a critical role in capturing a patient’s health history and providing a detailed record of conditions for clinical decision-making, research, and care management.\n",
    "\n",
    "| Column           | Description                                                                                 |\n",
    "|------------------|---------------------------------------------------------------------------------------------|\n",
    "| conditionId      | A unique identifier for the condition.                                                      |\n",
    "| patientId        | A unique identifier for the patient associated with the condition.                          |\n",
    "| encounterId      | The unique identifier for the encounter where the condition was diagnosed or recorded.      |\n",
    "| categoryCode     | A code representing the category of the condition (e.g., diagnosis, problem).               |\n",
    "| conditionCode    | The code that represents the specific condition, typically from a standardized code system. |\n",
    "| conditionDisplay | A human-readable display name for the condition.                                            |\n",
    "| conditionSystem  | The system from which the condition code is derived (e.g., ICD-10, SNOMED CT).              | -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_df = raw_df.filter(raw_df.sourceFile.isin(\"MimicCondition.ndjson.gz\", \"MimicConditionED.ndjson.gz\")) \n",
    "with open(\"schema/Condition.json\") as f: \n",
    "    schema_read = json.loads(f.read()) \n",
    "schema = T.StructType.fromJson(schema_read) \n",
    "condition_df = condition_df.withColumn(\"parsed_json\", F.from_json(condition_df[\"value\"], schema)) \n",
    "condition_df = condition_df.select(\"parsed_json.*\", \"sourceFile\") \n",
    "condition_df.createOrReplaceTempView(\"condition_df\") \n",
    "\n",
    "with open(\"sql_queries/silver.condition.load.sql\") as f:\n",
    "    fm_condition = spark.sql(f.read())\n",
    "\n",
    "fm_condition.show(5, truncate=False) \n",
    "fm_condition.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.silver.condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02. Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    spark.sql(\"select date_format(birthDate, 'yyyy') birthYear, count(*) count from local.silver.patient group by 1 order by 1\").toPandas(),\n",
    "    x='birthYear',\n",
    "    y='count',\n",
    "    title='Patient BirthYear Distribution [De-Identified]'\n",
    ").update_layout(\n",
    "    xaxis_title='Birth Year',\n",
    "    yaxis_title='Number of Patients',\n",
    "    template='plotly_white'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select gender, count(*) count from local.silver.patient group by 1 order by 1\").toPandas(),\n",
    "    values='count', names='gender', title='Gender Distribution'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar( \n",
    "    spark.sql(\"select gender, maritalStatus, count(*) count from local.silver.patient group by 1,2\").toPandas(), \n",
    "    x='maritalStatus', \n",
    "    y='count', \n",
    "    color='gender', \n",
    "    title='Count by Gender and Marital Status', \n",
    "    barmode='group' \n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encounter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    spark.sql(\"\"\"\n",
    "    select date_format(periodStart, 'yyyy') encounterYear, count(*) count from local.silver.encounter group by 1 order by 1\n",
    "    \"\"\").toPandas(),\n",
    "    x='encounterYear',\n",
    "    y='count',\n",
    "    title='Encounter Period Year Distribution [De-Identified]'\n",
    ").update_layout(\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Number of Encounters',\n",
    "    template='plotly_white'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select sourceFile, count(*) count from local.silver.encounter group by 1\").toPandas(),\n",
    "values='count', names='sourceFile', title='Source Distribution').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select encounterClass, count(*) count from local.silver.encounter group by 1\").toPandas(),\n",
    "values='count', names='encounterClass', title='Encounter Class Distribution').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select priority, count(*) count from local.silver.encounter group by 1\").toPandas(),\n",
    "values='count', names='priority', title='Encounter Class by Priority').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select readmissionStatus, count(*) count from local.silver.encounter group by 1\").toPandas(),\n",
    "values='count', names='readmissionStatus', title='Patient Readmission').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    spark.sql(\"\"\"select readmissionStatus, year(periodStart) year, count(*) count\n",
    "    from local.silver.encounter group by 1,2 order by 2\"\"\").toPandas(),\n",
    "    x='year',\n",
    "    y='count',\n",
    "    color='readmissionStatus', # Differentiate lines by readmissionStatus\n",
    "    title='Encounter Over the Years by ReadmissionStatus',\n",
    "    labels={'year': 'Year', 'count': 'Number of Encounters'}).update_layout(\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Number of Encounters',\n",
    "    legend_title='Readmission Status',\n",
    "    template='plotly_white'\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_encounter_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    e1.priority priority1,\n",
    "    e2.priority priority2,\n",
    "    count(*) count\n",
    "FROM local.silver.encounter e1\n",
    "JOIN local.silver.encounter e2\n",
    "ON e1.nextEncounterId = e2.encounterID\n",
    "WHERE e1.readmissionStatus == 'Readmission'\n",
    "GROUP BY all\n",
    "ORDER by 3 desc\n",
    "\"\"\")\n",
    "\n",
    "priority_encounter_pd = priority_encounter_df.toPandas()\n",
    "all_priorities = list(set(priority_encounter_pd['priority1'].tolist() +\n",
    "                          priority_encounter_pd['priority2'].tolist()))\n",
    "node_map = {priority: idx for idx, priority in enumerate(all_priorities)}\n",
    "priority_encounter_pd['source'] = priority_encounter_pd['priority1'].map(node_map)\n",
    "priority_encounter_pd['target'] = priority_encounter_pd['priority2'].map(node_map)\n",
    "\n",
    "table_trace = go.Table(\n",
    "    header=dict(values=[\"Priority 1\", \"Priority 2\", \"Count\"], fill_color='lightgrey', align='center'),\n",
    "    cells=dict(values=[priority_encounter_pd['priority1'], priority_encounter_pd['priority2'], priority_encounter_pd['count']],\n",
    "               fill_color='white', align='center')\n",
    ")\n",
    "\n",
    "sankey_trace = go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15, # Padding between nodes\n",
    "        thickness=20, # Node thickness\n",
    "        line=dict(color=\"black\", width=0.5), # Node border settings\n",
    "        label=all_priorities # Node labels\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=priority_encounter_pd['source'], # Source nodes (indices)\n",
    "        target=priority_encounter_pd['target'], # Target nodes (indices)\n",
    "        value=priority_encounter_pd['count'] # Flow values (counts)\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    column_widths=[0.7, 0.3], # Adjust column widths (30% table, 70% Sankey)\n",
    "    specs=[[{\"type\": \"table\"}, {\"type\": \"sankey\"}]], # Specify types for each subplot\n",
    ")\n",
    "\n",
    "\n",
    "fig.add_trace(sankey_trace, row=1, col=1)\n",
    "fig.add_trace(table_trace, row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    title_text=\"Readmission Encounters: Priority Transitions Sankey Diagram and Table\",\n",
    "    font_size=12,\n",
    "    height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT\n",
    "    sourceFile,\n",
    "    CASE\n",
    "        WHEN duration BETWEEN 0 AND 5 THEN '0 to 5 days'\n",
    "        WHEN duration BETWEEN 6 AND 10 THEN '6 to 10 days'\n",
    "        WHEN duration BETWEEN 11 AND 20 THEN '11 to 20 days'\n",
    "        WHEN duration BETWEEN 21 AND 30 THEN '21 to 30 days'\n",
    "        WHEN duration BETWEEN 31 AND 50 THEN '31 to 50 days'\n",
    "        WHEN duration BETWEEN 51 AND 100 THEN '51 to 100 days'\n",
    "        WHEN duration BETWEEN 101 AND 150 THEN '101 to 150 days'\n",
    "        WHEN duration BETWEEN 151 AND 200 THEN '151 to 200 days'\n",
    "        WHEN duration BETWEEN 201 AND 250 THEN '201 to 250 days'\n",
    "        WHEN duration BETWEEN 251 AND 300 THEN '251 to 300 days'\n",
    "        ELSE 'More than 300 days'\n",
    "    END AS duration_group,\n",
    "    COUNT(*) AS encounter_count\n",
    "FROM local.silver.encounter\n",
    "GROUP BY sourceFile, duration_group\n",
    "ORDER BY sourceFile, double(split_part(duration_group, ' ', 1));\n",
    "\"\"\").show(50, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.line(\n",
    "    spark.sql(\"\"\"SELECT\n",
    "        sourceFile,\n",
    "        duration AS duration_group,\n",
    "        COUNT(*) AS encounter_count\n",
    "    FROM local.silver.encounter\n",
    "    where duration BETWEEN 1 AND 30\n",
    "    GROUP BY sourceFile, duration_group\n",
    "    ORDER BY sourceFile, duration_group;\"\"\").toPandas(),\n",
    "    x='duration_group', y='encounter_count', color='sourceFile',\n",
    "    markers=True, title=\"Encounter Count by Source and Duration Group\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.pie(\n",
    "    spark.sql(\"select split_part(conditionSystem, '/', -1) conditionSystem, count(*) count from local.silver.condition group by 1\").toPandas(),\n",
    "    values='count', names='conditionSystem', title='Condition System Count Distribution').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the `Condition System Count Distribution` chart, the dataset incorporates both ICD-9 and ICD-10 coding systems in condition table.\n",
    "\n",
    "<!-- The difference between these two coding systems can introduce challenges which may affect the model performance and interpretability.\n",
    "\n",
    "1. Inconsistent Coding:\n",
    "    - ICD-10 has more granularity in specifying types of diseases, injury location, and severity.\n",
    "    - ICD-9 has fewer and less specific codes than ICD-10, which means that a single ICD-9 code could map to multiple ICD-10 codes.\n",
    "    - This inconsistency can create noisy features in your dataset if the same condition is coded differently depending on the coding system used. This could confuse your model, leading to reduced predictive accuracy.\n",
    "2. Feature Engineering Complexity:\n",
    "    - ICD-9 and ICD-10 are structured differently, both in terms of the number of codes and their specificity.\n",
    "    - This complicates the creation of features related to diagnosis categories or comorbidities.\n",
    "    - The features might not capture the full clinical picture, potentially leading to underfitting or overfitting\n",
    "3. Data Heterogeneity:\n",
    "    - Introduction of temporal bias into the model\n",
    "    - This could cause the model to overestimate or underestimate readmission risks if it correlates newer coding systems with better or worse outcomes.\n",
    "4. Model Interpretability:\n",
    "    - Mixed coding systems make model interpretation harder, especially if using interpretable models like decision trees or logistic regression.\n",
    "    - This may end up with features that are not comparable between ICD-9 and ICD-10, complicating efforts to explain your model's predictions.\n",
    "\n",
    "> **References**\n",
    "> - [Understanding the Impact of the Differences in ICD-9-CM and ICD10-CM and Its Potential Impact on Data Analysis | National Center for Health Statistics](https://www.cdc.gov/nchs/ppt/nchs2012/li-01_pickett.pdf)\n",
    "> - [Overview of Coding and Classification Systems](https://www.cms.gov/cms-guide-medical-technology-companies-and-other-interested-parties/coding/overview-coding-classification-systems) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select conditionCode, conditionDisplay, count(*) count from local.silver.condition\n",
    "where conditionSystem like '%mimic-diagnosis-icd10'\n",
    "group by all order by 3 desc\n",
    "\"\"\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "select conditionCode, conditionDisplay, count(*) count from local.silver.condition\n",
    "where conditionSystem like '%mimic-diagnosis-icd9'\n",
    "group by all order by 3 desc\n",
    "\"\"\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing top condition for both coding systems, there are few notable common types between them.\n",
    "\n",
    "For Example:\n",
    "\n",
    "- ICD 9 Code 4019 `Unspecified essential hypertension` is similar to ICD 10 Code I10 `Essential (primary) hypertension`.\n",
    "- ICD 9 Code V1582 `Personal history of tobacco use` is similar to ICD 10 Code Z87891 `Personal history of nicotine dependence`.\n",
    "- ICD 9 Code 2724 `Other and unspecified hyperlipidemia ` is similar to ICD 10 Code E785 `Hyperlipidemia, unspecified`.\n",
    "\n",
    "The dataset must be standardized in order to gain better accuracy of predictive model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 03. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Condition Domain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardarize Coding System\n",
    "\n",
    "<!-- The dataset, as shown in the Condition System Count Distribution chart, includes both ICD-9 and ICD-10 coding systems in the condition table.\n",
    "\n",
    "To standardize the dataset, one approach is to map ICD-9 codes to their ICD-10 equivalents using tools like General Equivalence Mappings (GEMs).\n",
    "\n",
    "This conversion ensures consistency across the dataset by aligning all condition codes with the ICD-10 system, which is the current standard for coding medical diagnoses and procedures.\n",
    "\n",
    "**ICD-10 Code Structure**\n",
    "- Characters 1:3 = Indicate the category of the diagnosis\n",
    "- Characters 4:6 = Indicate etiology, anatomic site, severity or other clinical detail\n",
    "- Character 7 = Extension\n",
    "\n",
    "> **References**\n",
    "> - [Crosswalk or General Equivalence Mappings | NBER](https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition-standard.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd10_df = spark.sql(\"\"\" \n",
    "    SELECT \n",
    "        conditionId, encounterId, patientId, conditionCode \n",
    "    FROM local.silver.condition condition \n",
    "    WHERE condition.conditionSystem LIKE '%icd10' \n",
    "\"\"\") \n",
    "icd10_df.write.format(\"iceberg\").mode(\"append\").save(\"local.silver.condition_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"dataset/icd9toicd10cmgem.csv\", header = True) \n",
    "df.createOrReplaceTempView(\"GemMapping\") \n",
    "\n",
    "with open(\"sql_queries/silver.condition-standard.load.sql\") as f:\n",
    "    icd9_df = spark.sql(f.read())\n",
    "\n",
    "icd9_df.write.format(\"iceberg\").mode(\"append\").save(\"local.silver.condition_standard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\" \n",
    "select \n",
    "    conditionID, encounterID, patientID, \n",
    "    conditionCode \n",
    "from local.silver.condition_standard \n",
    "\"\"\").show(10, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk Categorization\n",
    "\n",
    "<!-- **Risk categorization based on the Charlson Comorbidity Index (CCI)**\n",
    "\n",
    "Risk categorization based on the Charlson Comorbidity Index (CCI) is a widely used method for assessing the risk of adverse outcomes in patients, particularly in healthcare settings. The CCI assigns weights to various comorbid conditions based on their severity, such as heart disease, diabetes, and cancer. These conditions are then summed to produce a total score, which can be used to predict patient outcomes, including the risk of mortality and complications. The higher the CCI score, the greater the patient’s risk. This index helps clinicians and healthcare providers identify high-risk patients, prioritize care, and make informed decisions regarding treatment plans, resource allocation, and follow-up care. It is an essential tool for improving patient outcomes and optimizing healthcare management.\n",
    "\n",
    "| Condition                             | Weight | Risk Level |\n",
    "|---------------------------------------|--------|------------|\n",
    "| Peripheral vascular disease           | 1      | Low        |\n",
    "| Cerebrovascular disease               | 1      | Low        |\n",
    "| Chronic pulmonary disease             | 1      | Low        |\n",
    "| Congestive heart failure              | 1      | Low        |\n",
    "| Rheumatic disease                     | 1      | Low        |\n",
    "| Diabetes without chronic complication | 1      | Low        |\n",
    "| Mild liver disease                    | 1      | Low        |\n",
    "| Peptic ulcer disease                  | 1      | Low        |\n",
    "| Renal disease                         | 1      | Low        |\n",
    "| Myocardial infarction                 | 2      | Moderate   |\n",
    "| Hemiplegia or paraplegia              | 2      | Moderate   |\n",
    "| Malignancy                            | 2      | Moderate   |\n",
    "| Diabetes with chronic complication    | 2      | Moderate   |\n",
    "| Moderate or severe liver disease      | 3      | High       |\n",
    "| Metastatic solid tumour               | 6      | High       |\n",
    "| AIDS/HIV                              | 6      | High       |\n",
    "\n",
    "> **References**\n",
    "> - [Charlson Comorbidity Index Calculator](https://www.mdcalc.com/calc/3917/charlson-comorbidity-index-cci)\n",
    "> - [Calculation of the Charlson Comorbidity Index score](https://www.ncbi.nlm.nih.gov/books/NBK587905/)\n",
    "> - [ICD-10-CM to CMS-HCC Crosswalk](https://provider.amerigroup.com/dam/publicdocuments/ALL_CARE_CMSHCCRAModel_mrdcoding_tips.pdf)\n",
    "> - [CCI ICD-10 codes Mapping](https://www.psychiatryinvestigation.org/upload/media/pi-2021-0223-suppl1.pdf) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition-risk.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition-risk.load.sql\") as f:\n",
    "    condition_risk_df = spark.sql(f.read())\n",
    "\n",
    "condition_risk_df.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.silver.condition_risk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select charlsonCategory, riskLevel, count(*) from local.silver.condition_risk group by 1,2\").show(20, truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case-Based Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition-rollup.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/silver.condition-rollup.load.sql\") as f:\n",
    "    condition_rollup_df = spark.sql(f.read())\n",
    "\n",
    "condition_rollup_df.show(10, truncate = False)\n",
    "condition_rollup_df.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.silver.condition_rollup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidated Encounter\n",
    "\n",
    "<!-- The Gold Layer represents the refined stage in the Medallion Architecture, where data from the Silver Layer is consolidated and fully processed for advanced analytics and reporting. The Gold Table serves as the foundation for high-level analysis, enabling analysts, and researchers to derive meaningful insights that drive operational efficiency, improve patient outcomes, and support value-based care initiatives.\n",
    "\n",
    "The Encounter Gold Table integrates and structures the most critical data points related to patient encounters, ensuring that it is clean, accurate, and ready for decision-making. The table has various data points (such as patient demographics, encounter details, clinical diagnoses) merged and aggregated into a comprehensive, easily accessible format.  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/gold.encounter.ddl.sql\") as f:\n",
    "    spark.sql(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sql_queries/gold.encounter.load.sql\") as f:\n",
    "    gold_encounter_df = spark.sql(f.read())\n",
    "\n",
    "gold_encounter_df.write.format(\"iceberg\").mode(\"overwrite\").save(\"local.gold.encounter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04. Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquet_location = 'dataset/encounter.parquet'\n",
    "# gold_encounter_df = spark.read.format(\"iceberg\").load(\"local.gold.encounter\").toPandas()\n",
    "# gold_encounter_df.to_parquet(parquet_location)\n",
    "gold_encounter_df = pd.read_parquet(parquet_location)\n",
    "\n",
    "# Convert columns to the category data type. \n",
    "cols_to_convert = [\"gender\", \"ageGroup\", \"maritalStatus\", \"encounterStatus\", \"encounterClass\", \"admitSource\", \"encounterType\", \"priority\", \"readmissionStatus\"]\n",
    "for col in cols_to_convert:\n",
    "    gold_encounter_df[col] = gold_encounter_df[col].astype('category')\n",
    "\n",
    "non_features_columns = ['encounterId','patientId','encounterTs','encounterStatus', 'encounterType']\n",
    "gold_encounter_df = gold_encounter_df.drop(non_features_columns, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_encounter_df.info(verbose=False)\n",
    "gold_encounter_df.describe()\n",
    "s_datatype = pd.Series(gold_encounter_df.dtypes, name='DataType')\n",
    "s_uniquecount = pd.Series(gold_encounter_df.nunique(), name='Unique Count')\n",
    "s_count = pd.Series(gold_encounter_df.count(), name='Total Count')\n",
    "s_nullcount = pd.Series(gold_encounter_df.isnull().sum(), name='Null Values')\n",
    "report_df = pd.concat([s_datatype, s_uniquecount, s_count, s_nullcount], axis=1)\n",
    "display(report_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imbalance Ratio\n",
    "\n",
    "- An imbalance ratio of 1.54 is moderate and may still impact model performance.\n",
    "- Possible Cause: Readmissions (the minority class) are typically less frequent than non-readmissions (the majority class)\n",
    "- Impact of Class Imbalance: Bias toward Majority Class (Non-readmissions)\n",
    "\n",
    "Measures to solve imbalance ratio:\n",
    "- Undersampling the Majority Class: Remove some non-readmitted patients to balance the dataset. However, this may lead to the loss of useful information.\n",
    "- Oversampling the Minority Class: Increase the readmitted patients to balance the dataset. Help the model learn better about the minority class and prevent it from being biased toward the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distribution = gold_encounter_df.groupby('readmissionStatus').size()\n",
    "majority_class_count = target_distribution.max()\n",
    "minority_class_count = target_distribution.min()\n",
    "imbalance_ratio = majority_class_count/minority_class_count\n",
    "print(\"Imbalance Ratio: {:.2f}\".format(imbalance_ratio))\n",
    "\n",
    "target_distribution.plot(kind='bar')\n",
    "plt.title('Class Distribution')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xlabel('Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Readmission Status by Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by categorical variable and count occurrences\n",
    "grouped_data = {}\n",
    "for column in ['gender', 'ageGroup', 'maritalStatus', 'encounterClass', 'admitSource', 'priority']:\n",
    "    grouped_data[column] = gold_encounter_df.groupby([column, 'readmissionStatus']).size().reset_index(name='counts')\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 10))\n",
    "fig.suptitle('Count of Readmission Status by Categorical Variables', fontsize=16)\n",
    "for i, column in enumerate(grouped_data.keys()):\n",
    "    row = i // 3  # Calculate row index\n",
    "    col = i % 3   # Calculate column index\n",
    "    ax = axes[row, col]\n",
    "    for category in grouped_data[column][column].unique():\n",
    "        category_data = grouped_data[column][grouped_data[column][column] == category]\n",
    "        ax.bar(category_data['readmissionStatus'], category_data['counts'], label=category)\n",
    "    ax.set_xlabel('Readmission Status')\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'{column}')\n",
    "    ax.legend(title=column)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bar Chart Age Group Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gold_encounter_df\n",
    "order = ['young adults' ,'young adulthood' ,'early-middle age' ,'late-middle age' ,'mid-old age' ,'senior-old age' ,'very senior-old' ,'centenarians' ] \n",
    "\n",
    "df['ageGroup'] = pd.Categorical(df['ageGroup'], categories=order, ordered=True)\n",
    "grouped_df = df.groupby('ageGroup').size().reset_index(name='count')\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.bar(grouped_df['ageGroup'], grouped_df['count'])\n",
    "plt.xlabel('Age Group')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of Individuals by Age Group')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Risk Scores by Age Group And Rick Score Tens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gold_encounter_df[gold_encounter_df['riskScore']> 0]\n",
    "df['scoreTens'] = df['riskScore'] // 10 * 10\n",
    "\n",
    "grouped_df = df.groupby(['ageGroup', 'scoreTens']).size().reset_index(name='count')\n",
    "\n",
    "pivot_df = grouped_df.pivot_table(index='ageGroup', columns='scoreTens', values='count', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.imshow(pivot_df, cmap='viridis', interpolation='nearest')\n",
    "plt.xticks(range(len(pivot_df.columns)), pivot_df.columns)\n",
    "plt.yticks(range(len(pivot_df.index)), pivot_df.index)\n",
    "plt.xlabel('Risk Score Tens')\n",
    "plt.ylabel('Age Group')\n",
    "plt.title('Count of Risk Scores by Age Group and Risk Score Tens')\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(len(pivot_df.index)):\n",
    "    for j in range(len(pivot_df.columns)):\n",
    "        plt.text(j, i, str(pivot_df.values[i, j]), ha='center', va='center', color='w')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap High/Low/Moderate Risk by Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = ['highRiskCondition', 'moderateRiskCondition', 'lowRiskCondition']\n",
    "titles = ['High Risk', 'Moderate Risk', 'Low Risk']\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "for i, condition in enumerate(conditions):\n",
    "    filtered_df = df[df[condition] > 0]\n",
    "    filtered_df['scoreTens'] = filtered_df[condition]\n",
    "    grouped_df = filtered_df.groupby(['ageGroup', 'scoreTens']).size().reset_index(name='count')\n",
    "    pivot_df = grouped_df.pivot_table(index='ageGroup', columns='scoreTens', values='count', fill_value=0)\n",
    "\n",
    "    axes[i].imshow(pivot_df, cmap='viridis', interpolation='nearest')\n",
    "    axes[i].set_xticks(range(len(pivot_df.columns)), pivot_df.columns)\n",
    "    axes[i].set_yticks(range(len(pivot_df.index)), pivot_df.index)\n",
    "    axes[i].set_title(f'Count of {titles[i]} Scores by Age Group')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap High Risk by Age Group (Detail View)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gold_encounter_df[gold_encounter_df['highRiskCondition'] > 0]\n",
    "df['scoreTens'] = df['highRiskCondition']\n",
    "\n",
    "grouped_df = df.groupby(['ageGroup', 'scoreTens']).size().reset_index(name='count')\n",
    "\n",
    "pivot_df = grouped_df.pivot_table(index='ageGroup', columns='scoreTens', values='count', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.imshow(pivot_df, cmap='viridis', interpolation='nearest')\n",
    "plt.xticks(range(len(pivot_df.columns)), pivot_df.columns)\n",
    "plt.yticks(range(len(pivot_df.index)), pivot_df.index)\n",
    "plt.xlabel('High Risk Count')\n",
    "plt.ylabel('Age Group')\n",
    "plt.title('Count of High Risk by Age Group (Detail View)')\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(len(pivot_df.index)):\n",
    "    for j in range(len(pivot_df.columns)):\n",
    "        plt.text(j, i, str(pivot_df.values[i, j]), ha='center', va='center', color='w')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmap Length Stay by Age Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = gold_encounter_df[(gold_encounter_df['stayLength']> 0) & (gold_encounter_df['stayLength'] <= 60)]\n",
    "df['scoreTens'] = df['stayLength'] // 5 * 5\n",
    "\n",
    "grouped_df = df.groupby(['ageGroup', 'scoreTens']).size().reset_index(name='count')\n",
    "\n",
    "pivot_df = grouped_df.pivot_table(index='ageGroup', columns='scoreTens', values='count', fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.imshow(pivot_df, cmap='viridis', interpolation='nearest')\n",
    "plt.xticks(range(len(pivot_df.columns)), pivot_df.columns)\n",
    "plt.yticks(range(len(pivot_df.index)), pivot_df.index)\n",
    "plt.xlabel('Risk Score Tens')\n",
    "plt.ylabel('Age Group')\n",
    "plt.title('Count of Length Stay by Age Group')\n",
    "plt.colorbar()\n",
    "\n",
    "for i in range(len(pivot_df.index)):\n",
    "    for j in range(len(pivot_df.columns)):\n",
    "        plt.text(j, i, str(pivot_df.values[i, j]), ha='center', va='center', color='w')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_encounter_df['readmissionStatus'] = gold_encounter_df['readmissionStatus'].map({'Readmission': 1, 'No Readmission': 0})\n",
    "gold_encounter_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-Hot Encoding\n",
    "\n",
    "- Encoding categorical variables converts categorical variables (text labels) into numerical values (integer labels).\n",
    "- Many machine learning algorithms expect numerical input. Categorical variables, which represent groups or categories (e.g., \"red,\" \"blue,\" \"green\"), cannot be directly used by these algorithms.\n",
    "- **How it works**: For each category within a feature, a new binary column is created. A \"1\" is placed in the corresponding column for a specific data point if that category applies, and \"0\" otherwise.\n",
    "\n",
    "Why One-Hot Encoding over Label Encoding:\n",
    "- Avoids Implicit Ordinal Relationships (No Implicit Ranking)\n",
    "- Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['gender','ageGroup','maritalStatus','encounterClass','admitSource','priority']\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Apply one-hot encoding to the categorical columns\n",
    "one_hot_encoded = encoder.fit_transform(gold_encounter_df[categorical_columns])\n",
    "one_hot_df = pd.DataFrame(one_hot_encoded, columns=encoder.get_feature_names_out(categorical_columns))\n",
    "\n",
    "target_df = pd.concat([gold_encounter_df, one_hot_df], axis=1)\n",
    "target_df = target_df.drop(categorical_columns, axis=1)\n",
    "target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Oversampling the Minority Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = target_df.drop(columns=['readmissionStatus'])\n",
    "y = target_df['readmissionStatus']\n",
    "\n",
    "print(\"Class distribution before oversampling:\")\n",
    "print(Counter(y))\n",
    "\n",
    "# Apply SMOTE to oversample the minority class\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Check the class distribution after oversampling\n",
    "print(\"Class distribution after oversampling:\")\n",
    "print(Counter(y_resampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling Numerical Features\n",
    "\n",
    "- Ensures all numerical features in a dataset are on a similar scale.\n",
    "- Many machine learning algorithms are sensitive to the scale of the input features, particularly distance-based algorithms like k-Nearest Neighbors (k-NN) and gradient descent-based algorithms.\n",
    "- Features with larger scales can dominate the learning process, leading to biased models. Scaling ensures that all features contribute equally to the model's learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['stayLength', 'riskScore', 'highRiskCondition', 'moderateRiskCondition', 'lowRiskCondition'] \n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_resampled[cols_to_scale] = scaler.fit_transform(X_resampled[cols_to_scale])\n",
    "\n",
    "X_resampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pearson Correlation Coefficient\n",
    "\n",
    "> Moderate positive correlation between `risk score` and `risk conditions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = X_resampled[[\"stayLength\",\"riskScore\",\"highRiskCondition\",\"moderateRiskCondition\",\"lowRiskCondition\"]]\n",
    "correlation_matrix = df.corr(method='pearson')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", cbar=True)\n",
    "plt.title('Correlation Matrix Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_resampled, y_resampled, test_size=0.275, random_state=123\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
