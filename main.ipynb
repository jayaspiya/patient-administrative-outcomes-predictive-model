{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_BUCKET = \"../data_bucket/physionet_extract\"\n",
    "for item in os.listdir(DATA_BUCKET):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"FhirDataApplication\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Encounter Data with Infer Schema\n",
    "- Analyze Number of Columns in Each File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MimicEncounter = spark.read.json(f\"{DATA_BUCKET}/MimicEncounter.ndjson\")\n",
    "df_MimicEncounterED = spark.read.json(f\"{DATA_BUCKET}/MimicEncounterED.ndjson\")\n",
    "df_MimicEncounterICU = spark.read.json(f\"{DATA_BUCKET}/MimicEncounterICU.ndjson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recursive_print_schema(schema, parent = \"\"):\n",
    "    for item in schema['fields']:\n",
    "        name = item['name']\n",
    "        type = item['type']\n",
    "        if isinstance(type, dict):\n",
    "            if type.get('type') == 'struct':\n",
    "                recursive_print_schema(type, f\"{parent}>{name}\")           \n",
    "            elif type.get('type') == 'array':\n",
    "                if isinstance(type.get('elementType'), dict):\n",
    "                    recursive_print_schema(type.get('elementType'), f\"{parent}>{name}\")\n",
    "                else:\n",
    "                    print(f\"{parent}>{name}\")\n",
    "            else:\n",
    "                print(\"<<<<<>>>>>\")\n",
    "        else:\n",
    "            print(f\"{parent}>{name}\")\n",
    "    \n",
    "\n",
    "# recursive_print_schema(df_MimicEncounterICU.schema.jsonValue())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_column_nested = spark.read.csv(\"./ColumnNested.csv\", header=True)\n",
    "df_column_nested.createOrReplaceTempView(\"df_column_nested\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sparksql -l 50\n",
    "SELECT\n",
    "    ColumnNested, collect_list(File) files, count(*) num_of_sources\n",
    "FROM df_column_nested\n",
    "GROUP BY 1\n",
    "ORDER BY 3 desc, 1 asc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read all Encounter Data in single dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Encounter JSON Schema\n",
    "with open(f\"./schema/Encounter.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "    encounter_schema = T.StructType.fromJson(schema_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_MimicEncounter = spark.read.schema(encounter_schema).json(f\"{DATA_BUCKET}/MimicEncounter.ndjson\").withColumn(\"source_name\", F.lit(\"MimicEncounter\"))\n",
    "df_MimicEncounterED = spark.read.schema(encounter_schema).json(f\"{DATA_BUCKET}/MimicEncounterED.ndjson\").withColumn(\"source_name\", F.lit(\"MimicEncounterED\"))\n",
    "df_MimicEncounterICU = spark.read.schema(encounter_schema).json(f\"{DATA_BUCKET}/MimicEncounterICU.ndjson\").withColumn(\"source_name\", F.lit(\"MimicEncounterICU\"))\n",
    "\n",
    "df_encounter = df_MimicEncounter.unionAll(df_MimicEncounterED).unionAll(df_MimicEncounterICU)\n",
    "df_encounter.createOrReplaceTempView(\"df_encounter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "SELECT\n",
    "    source_name, count(*) cnt\n",
    "FROM df_encounter\n",
    "GROUP BY 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "-- Encounter Class of Each DF\n",
    "-- https://hl7.org/fhir/R4/v3/ActEncounterCode/vs.html\n",
    "SELECT\n",
    "    source_name,\n",
    "    class.code encounter_class,\n",
    "    count(1) cnt\n",
    "FROM df_encounter GROUP BY 1,2\n",
    "ORDER BY 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "-- Code System\n",
    "-- AdmitSource: https://mimic.mit.edu/fhir/ValueSet-mimic-admit-source.html\n",
    "-- DischargeDisposition: https://mimic.mit.edu/fhir/ValueSet-mimic-discharge-disposition.html\n",
    "SELECT distinct \"admitSource\" key, explode(hospitalization.admitSource.coding.code) value from df_MimicEncounter UNION\n",
    "SELECT distinct \"dischargeDisposition\" key, explode(hospitalization.dischargeDisposition.coding.code) value from df_MimicEncounter\n",
    "ORDER BY 1,2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql -l 10\n",
    "SELECT distinct\n",
    "hospitalization.admitSource.coding.code[0] admitSource,\n",
    "hospitalization.dischargeDisposition.coding.code[0] dischargeDisposition\n",
    "from df_MimicEncounter\n",
    "order by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select distinct status FROM df_Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "-- partOf>reference\n",
    "with temp as (\n",
    "    select id, replace(partOf.reference, \"Encounter/\", \"\") parent_id, source_name\n",
    "    FROM df_Encounter\n",
    "    where partOf.reference is not null\n",
    ")\n",
    "select\n",
    "    -- temp.id,\n",
    "    -- temp.parent_id,\n",
    "    df_Encounter.source_name parent_source,\n",
    "    temp.source_name,\n",
    "    count(*)\n",
    "from temp\n",
    "left join df_Encounter\n",
    "on df_Encounter.id = temp.parent_id\n",
    "group by 1,2\n",
    "limit 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "--  Check if the Encounter have GrandParents\n",
    "with temp as (\n",
    "    select id,\n",
    "    replace(partOf.reference, \"Encounter/\", \"\") parent_id\n",
    "    FROM df_Encounter\n",
    "    where partOf.reference is not null\n",
    ")\n",
    "select\n",
    "distinct partOf.reference\n",
    "FROM df_Encounter\n",
    "where id in (select parent_id from temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select\n",
    "replace(partOf.reference, \"Encounter/\", \"\") parent_id, count(*) cnt\n",
    "FROM df_Encounter\n",
    "where partOf.reference is not null\n",
    "group by 1\n",
    "order by 2 desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select type_coding.* from \n",
    "(select  explode(type_coding) type_coding from (\n",
    "select explode(type.coding) type_coding from df_encounter\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "select serviceType from df_encounter\n",
    "where id = 'c8816dcb-47f0-55f0-933f-122d0caac629'\n",
    "-- Encounter Class: Emergency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%%sparksql\n",
    "with encounter as (\n",
    "    select\n",
    "        id,\n",
    "        class.code,\n",
    "        replace(partOf.reference, \"Encounter/\", \"\") parent_id,\n",
    "        CAST(period.start AS Date) period_start,\n",
    "        CAST(period.end AS Date)   period_end\n",
    "    from df_encounter\n",
    "),\n",
    "encounter_parent_child as (\n",
    "    select \n",
    "        parent_encounter.id              AS parent_encounter_id,      \n",
    "        parent_encounter.code            AS parent_encounter_code,        \n",
    "        parent_encounter.period_start    AS parent_encounter_period_start,                \n",
    "        parent_encounter.period_end      AS parent_encounter_period_end,\n",
    "        (parent_encounter.period_end - parent_encounter.period_start) parent_encounter_duration,          \n",
    "        child_encounter.id               AS child_encounter_id,     \n",
    "        child_encounter.code             AS child_encounter_code,       \n",
    "        child_encounter.period_start     AS child_encounter_period_start,               \n",
    "        child_encounter.period_end       AS child_encounter_period_end,\n",
    "        (child_encounter.period_end - child_encounter.period_start) child_encounter_duration,\n",
    "        (parent_encounter.period_end - child_encounter.period_start) duration_between_parent_child\n",
    "    from encounter child_encounter\n",
    "    left join encounter parent_encounter\n",
    "    on child_encounter.parent_id = parent_encounter.id\n",
    "    where child_encounter.parent_id is not null\n",
    ")\n",
    "select * from encounter_parent_child\n",
    "-- select parent_encounter_code source,child_encounter_code target, count(*) cnt from encounter_parent_child group by all order by 3 desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encounter_source_target_data = spark.sql(\"\"\"\n",
    "with encounter as (\n",
    "    select\n",
    "        id,\n",
    "        class.code,\n",
    "        replace(partOf.reference, \"Encounter/\", \"\") parent_id,\n",
    "        CAST(period.start AS Date) period_start,\n",
    "        CAST(period.end AS Date)   period_end\n",
    "    from df_encounter\n",
    "),\n",
    "encounter_parent_child as (\n",
    "    select \n",
    "        parent_encounter.id              AS parent_encounter_id,      \n",
    "        parent_encounter.code            AS parent_encounter_code,        \n",
    "        parent_encounter.period_start    AS parent_encounter_period_start,                \n",
    "        parent_encounter.period_end      AS parent_encounter_period_end,\n",
    "        (parent_encounter.period_end - parent_encounter.period_start) parent_encounter_duration,          \n",
    "        child_encounter.id               AS child_encounter_id,     \n",
    "        child_encounter.code             AS child_encounter_code,       \n",
    "        child_encounter.period_start     AS child_encounter_period_start,               \n",
    "        child_encounter.period_end       AS child_encounter_period_end,\n",
    "        (child_encounter.period_end - child_encounter.period_start) child_encounter_duration,\n",
    "        (parent_encounter.period_end - child_encounter.period_start) duration_between_parent_child\n",
    "    from encounter child_encounter\n",
    "    left join encounter parent_encounter\n",
    "    on child_encounter.parent_id = parent_encounter.id\n",
    "    where child_encounter.parent_id is not null\n",
    ")\n",
    "select parent_encounter_code source,child_encounter_code target, count(*) cnt from encounter_parent_child group by all order by 3 desc\n",
    "\"\"\").collect()\n",
    "\n",
    "source = [item['source'] for item in encounter_source_target_data]\n",
    "target = [item['target'] for item in encounter_source_target_data]\n",
    "count = [item['cnt'] for item in encounter_source_target_data]\n",
    "\n",
    "# Create a list of unique labels\n",
    "labels = list(set(source + target))\n",
    "\n",
    "# Create a mapping from labels to indices\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Convert source and target labels to indices\n",
    "source_indices = [label_to_index[s] for s in source]\n",
    "target_indices = [label_to_index[t] for t in target]\n",
    "\n",
    "# Create the Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=labels\n",
    "    ),\n",
    "    link=dict(\n",
    "        source=source_indices,\n",
    "        target=target_indices,\n",
    "        value=count\n",
    "    )\n",
    ")])\n",
    "\n",
    "# Update the layout and display the figure\n",
    "fig.update_layout(title_text=\"Ecnounter Class Sankey Diagram\", font_size=10)\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
