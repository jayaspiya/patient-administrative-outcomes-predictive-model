{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Administrative Outcomes Predictive Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Project Init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.3.tar.gz (317.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:06\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting py4j==0.10.9.7 (from pyspark)\n",
      "  Downloading py4j-0.10.9.7-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Downloading py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pyspark: filename=pyspark-3.5.3-py2.py3-none-any.whl size=317840629 sha256=d80d9c80d08f0ded5c0b65cddfc910a09e9c392f35cbd80f559b485afb0ed626\n",
      "  Stored in directory: /home/snowblade/.cache/pip/wheels/07/a0/a3/d24c94bf043ab5c7e38c30491199a2a11fef8d2584e6df7fb7\n",
      "Successfully built pyspark\n",
      "Installing collected packages: py4j, pyspark\n",
      "Successfully installed py4j-0.10.9.7 pyspark-3.5.3\n"
     ]
    }
   ],
   "source": [
    "# !pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Pyspark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FhirDataApplication</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7d4d8000efc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "import json\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FhirDataApplication\").getOrCreate()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Patient Resource\n",
    "- `patientID`: Unique identifier for the patient\n",
    "- `gender`: Gender of the patient (male/female)\n",
    "- `birthDate`: Birth Date\n",
    "- `maritalStatus`: Marital Status "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/10/28 00:08:59 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "with open(\"schema/Patient.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicPatient =  spark.read.schema(schema).json(\"_dataset/MimicPatient.ndjson.gz\")\n",
    "    \n",
    "df_MimicPatient.createOrReplaceTempView(\"df_patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------+----------+-------------+\n",
      "|patientId                           |gender|birthDate |maritalStatus|\n",
      "+------------------------------------+------+----------+-------------+\n",
      "|c1a51556-8f1b-5ca1-a8b5-ece5e7bb8602|male  |2048-07-11|M            |\n",
      "|363f1c3f-87b7-54f9-b22e-d9ed1bc03906|female|2131-12-18|UNK          |\n",
      "|e0461572-bda1-5fc1-9703-607152294019|male  |2091-06-06|UNK          |\n",
      "|d16c5811-06c5-5f7f-87d7-fe2adb7bd10c|male  |2105-04-05|UNK          |\n",
      "|7458d66f-3fd9-5208-9223-4fdf0bacbfb8|male  |2129-02-16|UNK          |\n",
      "+------------------------------------+------+----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm_patient = spark.sql(\"\"\"\n",
    "select \n",
    "    id AS patientId,\n",
    "    gender AS gender,\n",
    "    to_date(birthDate) birthDate,\n",
    "    maritalStatus.coding[0].code AS maritalStatus\n",
    "from df_patient\n",
    "\"\"\")\n",
    "fm_patient.createOrReplaceTempView('fm_patient')\n",
    "fm_patient.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fm_patient.write.mode(\"overwrite\").parquet(\"_output/fm_patient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Encounter\n",
    "\n",
    "The Encounter is a resource that represents an interaction between a patient and healthcare provider(s) for the purpose of providing healthcare services or assessing the patient's health status.\n",
    "\n",
    "It records the full span of a hospital stay, including admission, stay and discharge. It includes details such as admission start and end time, context for the admission and patient movements within the hospital.\n",
    "\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `encounterId`: Unique identifier for the encounter.\n",
    "- `patientId`: Unique identifier for the patient.\n",
    "- `ref_encounterId`:Reference to a related encounter; can reference both future and past encounters.\n",
    "- `periodStart`: Start timestamp of the encounter period.\n",
    "- `periodEnd`:End timestamp of the encounter period.\n",
    "- `duration`: Total duration of encounter\n",
    "- `status`: Current status of the encounter (e.g., planned, in-progress, finished).\n",
    "- `encounterClass`: Classification of the encounter (e.g., inpatient, outpatient); Helps categorize the nature of the healthcare service provided.\n",
    "- `codedType`: Code representing the specific type of encounter.\n",
    "- `displayType`: Display name for the type of encounter.\n",
    "- `systemType`: System from which the type code is derived.\n",
    "- `priority`: Urgency of the encounter such as routine, urgent, or emergency; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Encounter.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicEncounter =  spark.read.schema(schema).json(\"_dataset/MimicEncounter.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicEncounterED =  spark.read.schema(schema).json(\"_dataset/MimicEncounterED.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_encounter = df_MimicEncounter.union(df_MimicEncounterED)\n",
    "df_encounter.createOrReplaceTempView(\"df_encounter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------------+--------+--------+--------------+---------------------------+---------+------------------------------------+-----------------+--------------------------+\n",
      "|encounterId                         |patientId                           |ref_encounterId                     |periodStart        |periodEnd          |duration|status  |encounterClass|displayType                |priority |nextEncounterId                     |readmissionStatus|sourceName                |\n",
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------------+--------+--------+--------------+---------------------------+---------+------------------------------------+-----------------+--------------------------+\n",
      "|c2c18e9c-1177-5b71-aa93-e1eae57868b5|0000ad47-7103-5f54-970d-dafc42fd12f9|NULL                                |2141-08-24 20:37:00|2141-08-24 23:19:00|0       |finished|EMER          |Patient encounter procedure|emergency|338f50e6-7d1d-5adc-8884-6f557a3e3d10|Readmission      |MimicEncounterED.ndjson.gz|\n",
      "|338f50e6-7d1d-5adc-8884-6f557a3e3d10|0000ad47-7103-5f54-970d-dafc42fd12f9|NULL                                |2141-08-27 20:25:00|2141-08-27 22:59:00|0       |finished|EMER          |Patient encounter procedure|emergency|fcd20813-83be-5a74-ad38-adab31082bcf|No Readmission   |MimicEncounterED.ndjson.gz|\n",
      "|fcd20813-83be-5a74-ad38-adab31082bcf|0000ad47-7103-5f54-970d-dafc42fd12f9|NULL                                |2142-10-16 18:21:00|2142-10-16 22:05:00|0       |finished|EMER          |Patient encounter procedure|emergency|7934255b-2d65-52a5-a6ae-4842f955b51a|Readmission      |MimicEncounterED.ndjson.gz|\n",
      "|7934255b-2d65-52a5-a6ae-4842f955b51a|0000ad47-7103-5f54-970d-dafc42fd12f9|c7eb77e5-0dcd-57fb-8393-3b737ae974ed|2142-10-18 00:36:00|2142-10-18 09:46:00|0       |finished|EMER          |Patient encounter procedure|emergency|c7eb77e5-0dcd-57fb-8393-3b737ae974ed|Readmission      |MimicEncounterED.ndjson.gz|\n",
      "|c7eb77e5-0dcd-57fb-8393-3b737ae974ed|0000ad47-7103-5f54-970d-dafc42fd12f9|NULL                                |2142-10-18 07:34:00|2142-10-20 21:42:00|2       |finished|EMER          |Patient encounter procedure|emergency|660b8a35-9ae1-54e5-9b96-3303e1b7b7c4|No Readmission   |MimicEncounter.ndjson.gz  |\n",
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------------+--------+--------+--------------+---------------------------+---------+------------------------------------+-----------------+--------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fm_encounter = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS encounterId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(partOf.reference, \"Encounter/\", \"\") AS ref_encounterId,\n",
    "    CAST(period.start AS timestamp) periodStart,\n",
    "    CAST(period.end AS timestamp) periodEnd,\n",
    "    date_diff(day, periodStart, periodEnd) duration,\n",
    "    status AS status,\n",
    "    class.code AS encounterClass,\n",
    "    type[0].coding[0].display AS displayType,\n",
    "    nvl(priority.coding[0].display, 'emergency') AS priority,\n",
    "    -- Next EncounterID & Readmission Status\n",
    "    LEAD(id) OVER (PARTITION BY subject.reference ORDER BY period.start) AS nextEncounterId,\n",
    "    CASE \n",
    "        WHEN DATEDIFF(day, period.end, LEAD(period.start) OVER (PARTITION BY subject.reference ORDER BY period.start)) <= 30 THEN 'Readmission'\n",
    "        ELSE 'No Readmission'\n",
    "    END AS readmissionStatus,\n",
    "    sourceName\n",
    "FROM df_encounter\n",
    "\"\"\")\n",
    "fm_encounter.createOrReplaceTempView('fm_encounter')\n",
    "fm_encounter.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fm_encounter.write.mode(\"overwrite\").parquet(\"_output/fm_encounter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Condition\n",
    "\n",
    "The Condition resource in FHIR is used to record detailed information about a patient’s health state, including diagnoses, problems, or other clinical concerns. Here are the key points:\n",
    "\n",
    "- Scope and Usage: It captures conditions that have risen to a level of concern, such as diseases, health issues, or post-procedure states.\n",
    "- Clinical Context: Conditions can be recorded based on a clinician’s assessment or expressed by the patient or care team members.\n",
    "- Examples: Conditions like pregnancy, post-surgical states, or chronic illnesses can be documented. It can also include social determinants of health like unemployment or lack of transportation.\n",
    "\n",
    "Data Preprocessing Condition Code System is both ICD 9 & ICD 10, Standardize ICD-9 and ICD-10 codes to a common standard. GEMs (General Equivalence Mappings) are crosswalks between ICD-9 and ICD-10 codes. They help map codes from ICD-9-CM to ICD-10-CM and vice versa.\n",
    "\n",
    "https://www.cms.gov/Medicare/Coding/ICD10/Downloads/ICD-10_GEM_fact_sheet.pdf\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `conditionId`: The unique identifier for the condition.\n",
    "- `patientId`: The unique identifier for the patient.\n",
    "- `encounterId`: The unique identifier for the encounter.\n",
    "- `categoryCode`: Condition category.\n",
    "- `conditionCode`: The code representing the specific condition.\n",
    "- `conditionDisplay`: The display name for the condition.\n",
    "- `conditionSystem`: The system from which the condition code is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Condition.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicCondition =  spark.read.schema(schema).json(\"_dataset/MimicCondition.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicConditionED =  spark.read.schema(schema).json(\"_dataset/MimicConditionED.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_condition = df_MimicCondition.union(df_MimicConditionED)\n",
    "#  Remove Corrupt Dataset\n",
    "df_condition = df_condition.filter(df_condition[\"id\"].isNotNull())\n",
    "df_condition.createOrReplaceTempView(\"df_condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------+--------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "|conditionId                         |patientId                           |encounterId                         |categoryCode       |conditionCode|conditionDisplay                                  |conditionSystem                                                |sourceName              |\n",
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------+--------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "|10ce1ed4-c6cc-59b7-99b3-f16d355642c3|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|encounter-diagnosis|7455         |Ostium secundum type atrial septal defect         |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9|MimicCondition.ndjson.gz|\n",
      "|2aedfdd4-5f41-50bb-94a3-2e39825384c2|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|encounter-diagnosis|53081        |Esophageal reflux                                 |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9|MimicCondition.ndjson.gz|\n",
      "|250944ab-186c-5413-8ccb-0541d7ede098|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|encounter-diagnosis|V1389        |Personal history of other specified diseases      |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9|MimicCondition.ndjson.gz|\n",
      "|5b80c8c2-a1dc-52a2-ae1a-08ac170a369d|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|encounter-diagnosis|41401        |Coronary atherosclerosis of native coronary artery|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9|MimicCondition.ndjson.gz|\n",
      "|eb737eac-7bd4-5692-8418-8fa59d4d5dc9|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|encounter-diagnosis|5853         |Chronic kidney disease, Stage III (moderate)      |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9|MimicCondition.ndjson.gz|\n",
      "+------------------------------------+------------------------------------+------------------------------------+-------------------+-------------+--------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm_condition = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS conditionId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(encounter.reference, \"Encounter/\", \"\") AS encounterId,\n",
    "    category[0].coding[0].code categoryCode,\n",
    "    code.coding[0].code AS conditionCode,\n",
    "    code.coding[0].display AS conditionDisplay,\n",
    "    code.coding[0].system AS conditionSystem,\n",
    "    sourceName\n",
    "FROM df_condition\n",
    "\"\"\")\n",
    "fm_condition.createOrReplaceTempView('fm_condition')\n",
    "fm_condition.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fm_condition.write.mode(\"overwrite\").parquet(\"_output/fm_condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Procedure\n",
    "\n",
    "The Procedure resource in FHIR is used to record details of current and historical procedures performed on or for a patient.\n",
    "\n",
    "**Table Attributes**\n",
    "\n",
    "- `procedureId`: The unique identifier for the procedure.\n",
    "- `patientId`: The unique identifier for the patient.\n",
    "- `encounterId`: The unique identifier for the encounter.\n",
    "- `status`: The current status of the procedure (e.g., completed, in-progress, not-done)\n",
    "- `performedDateTime`: The date and time when the procedure was performed.\n",
    "- `procedureCode`: The code representing the specific procedure.\n",
    "- `procedureDisplay`: The display name for the procedure.\n",
    "- `procedureSystem`: The system from which the procedure code is derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"schema/Procedure.json\") as f:\n",
    "    schema_read = json.loads(f.read())\n",
    "schema =  T.StructType.fromJson(schema_read)\n",
    "df_MimicProcedure =  spark.read.schema(schema).json(\"_dataset/MimicProcedure.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_MimicProcedureED =  spark.read.schema(schema).json(\"_dataset/MimicProcedureED.ndjson.gz\") \\\n",
    "    .withColumn(\"sourceName\", F.split_part(F.input_file_name(), F.lit(\"/\"), F.lit(-1)))\n",
    "\n",
    "df_procedure = df_MimicProcedure.union(df_MimicProcedureED)\n",
    "df_procedure.createOrReplaceTempView(\"df_procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+------------------------------------+------------------------------------+---------+-----------------+-------------+--------------------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "|procedureId                         |patientId                           |encounterId                         |status   |performedDateTime|procedureCode|procedureDisplay                                              |procedureSystem                                                |sourceName              |\n",
      "+------------------------------------+------------------------------------+------------------------------------+---------+-----------------+-------------+--------------------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "|06572895-c093-5678-a2ee-52025f3fe961|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|completed|2140-01-06       |3615         |Single internal mammary-coronary artery bypass                |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9|MimicProcedure.ndjson.gz|\n",
      "|18346b99-0966-5d74-ae19-9b2b86d9a184|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|completed|2140-01-06       |3961         |Extracorporeal circulation auxiliary to open heart surgery    |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9|MimicProcedure.ndjson.gz|\n",
      "|ca291152-7892-53ed-b596-79a7bb79a001|00000027-c5e0-554f-8e85-b097c3b177d4|c8816dcb-47f0-55f0-933f-122d0caac629|completed|2140-01-06       |3571         |Other and unspecified repair of atrial septal defect          |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9|MimicProcedure.ndjson.gz|\n",
      "|7bb139d6-2588-532d-8b90-cac916fcee8b|0000c20f-4079-5da2-a3ed-0a5118ec6184|36f04da9-3155-5804-b53c-9fbed5b4797f|completed|2150-12-24       |8919         |Video and radio-telemetered electroencephalographic monitoring|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9|MimicProcedure.ndjson.gz|\n",
      "|14525c26-b2a3-5ac0-b691-5fc6be6602aa|00011d50-5301-59a9-a134-88499850696a|19470518-ed38-5cf0-b986-2e3066494742|completed|2162-09-01       |5185         |Endoscopic sphincterotomy and papillotomy                     |http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9|MimicProcedure.ndjson.gz|\n",
      "+------------------------------------+------------------------------------+------------------------------------+---------+-----------------+-------------+--------------------------------------------------------------+---------------------------------------------------------------+------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fm_procedure = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    id AS procedureId,\n",
    "    replace(subject.reference, \"Patient/\", \"\") AS patientId,\n",
    "    replace(encounter.reference, \"Encounter/\", \"\") AS encounterId,\n",
    "    status AS status,\n",
    "    nvl(to_date(performedDateTime), to_date(performedPeriod.start)) AS performedDateTime,\n",
    "    code.coding[0].code AS procedureCode,\n",
    "    code.coding[0].display AS procedureDisplay,\n",
    "    code.coding[0].system AS procedureSystem,\n",
    "    sourceName\n",
    "FROM df_procedure\n",
    "\"\"\")\n",
    "fm_procedure.createOrReplaceTempView('fm_procedure')\n",
    "fm_procedure.show(5, truncate = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# fm_procedure.write.mode(\"overwrite\").parquet(\"_output/fm_procedure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Standardarize Condition Coding System\n",
    "\n",
    "There are two coding system in condition domain: ICD-9 & ICD-10.\n",
    "\n",
    "To facilitate transition, CMS and CDC developed GEMs, which act as crosswalks, translating ICD-9 codes to their ICD-10 equivalents.\n",
    "\n",
    "Learn more about this crosswalk on [ICD-9-CM to and from ICD-10-CM and ICD-10-PCS Crosswalk or General Equivalence Mappings](https://www.nber.org/research/data/icd-9-cm-and-icd-10-cm-and-icd-10-pcs-crosswalk-or-general-equivalence-mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+--------+\n",
      "|conditionSystem                                                 |count(1)|\n",
      "+----------------------------------------------------------------+--------+\n",
      "|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd9 |3209891 |\n",
      "|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-diagnosis-icd10|2445484 |\n",
      "+----------------------------------------------------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT conditionSystem, count(*) FROM fm_condition\n",
    "group by 1\n",
    "order by 1 desc\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICD-10 Code Structure\n",
    "- Characters 1:3 = Indicate the category of the diagnosis\n",
    "- Characters 4:6 = Indicate etiology, anatomic site, severity or other clinical detail\n",
    "- Character 7 = Extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ClinicalModificationGem_df = spark.read.csv(\"_dataset/icd9toicd10cmgem.csv\", header = True)\n",
    "ClinicalModificationGem_df.createOrReplaceTempView(\"ClinicalModificationGem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_condition_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM fm_condition condition \n",
    "    WHERE condition.conditionSystem LIKE '%icd10'\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        condition.conditionId,\n",
    "        condition.patientId,\n",
    "        condition.encounterId,\n",
    "        condition.categoryCode,\n",
    "        ClinicalModificationGem.icd10cm AS conditionCode,\n",
    "        CAST(NULL AS STRING) conditionDisplay,\n",
    "        CAST(NULL AS STRING) AS conditionSystem,\n",
    "        condition.sourceName\n",
    "    FROM fm_condition condition \n",
    "    JOIN ClinicalModificationGem\n",
    "    ON condition.conditionCode = ClinicalModificationGem.icd9cm\n",
    "    WHERE condition.conditionSystem LIKE '%icd9'\n",
    "\"\"\")\n",
    "\n",
    "standardize_condition_df.createOrReplaceTempView(\"ICD10Condition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition_pivot_df = spark.sql(\"\"\"\n",
    "    WITH RankedConditions AS (\n",
    "        SELECT \n",
    "            patientId,\n",
    "            encounterId,\n",
    "            conditionCode,\n",
    "            COUNT(*) AS conditionCount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY encounterId ORDER BY COUNT(*) DESC) AS rank\n",
    "        FROM ICD10Condition\n",
    "        GROUP BY patientId, encounterId, conditionCode\n",
    "    )\n",
    "    SELECT\n",
    "        patientId,\n",
    "        encounterId,\n",
    "        MAX(CASE WHEN rank = 1 THEN conditionCode END) AS condition_1,\n",
    "        MAX(CASE WHEN rank = 2 THEN conditionCode END) AS condition_2,\n",
    "        MAX(CASE WHEN rank = 3 THEN conditionCode END) AS condition_3\n",
    "    FROM RankedConditions\n",
    "    GROUP BY patientId, encounterId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# condition_pivot_df.write.mode(\"overwrite\").parquet(\"_output/fm2_condition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Standardarize Procedure Coding System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+--------+\n",
      "|procedureSystem                                                 |count(1)|\n",
      "+----------------------------------------------------------------+--------+\n",
      "|http://snomed.info/sct                                          |1989697 |\n",
      "|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd9 |446079  |\n",
      "|http://mimic.mit.edu/fhir/mimic/CodeSystem/mimic-procedure-icd10|223107  |\n",
      "+----------------------------------------------------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT procedureSystem, count(*) FROM fm_procedure\n",
    "group by 1\n",
    "order by 1 desc\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------+\n",
      "|procedureDisplay                                 |\n",
      "+-------------------------------------------------+\n",
      "|Taking patient vital signs assessment (procedure)|\n",
      "|Triage: emergency center (procedure)             |\n",
      "+-------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "select distinct procedureDisplay from fm_procedure\n",
    "where procedureSystem like '%sct'       \n",
    "\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "ProcedureCodingSystemGem_df = spark.read.csv(\"_dataset/icd9toicd10pcsgem.csv\", header = True)\n",
    "ProcedureCodingSystemGem_df.createOrReplaceTempView(\"ProcedureCodingSystemGem\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardize_procedure_df = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        *\n",
    "    FROM fm_procedure procedure \n",
    "    WHERE procedure.procedureSystem LIKE '%icd10'\n",
    "    \n",
    "    UNION\n",
    "    \n",
    "    SELECT\n",
    "        procedure.procedureId,\n",
    "        procedure.patientId,\n",
    "        procedure.encounterId,\n",
    "        procedure.status,\n",
    "        procedure.performedDateTime,\n",
    "        ProcedureCodingSystemGem.icd10cm AS procedureCode,\n",
    "        CAST(NULL AS STRING) procedureDisplay,\n",
    "        CAST(NULL AS STRING) AS procedureSystem,\n",
    "        procedure.sourceName\n",
    "    FROM fm_procedure procedure \n",
    "    JOIN ProcedureCodingSystemGem\n",
    "    ON procedure.procedureCode = ProcedureCodingSystemGem.icd9cm\n",
    "    WHERE procedure.procedureSystem LIKE '%icd9'\n",
    "\"\"\")\n",
    "\n",
    "standardize_procedure_df.createOrReplaceTempView(\"ICD10procedure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "procedure_pivot_df = spark.sql(\"\"\"\n",
    "    WITH Rankedprocedures AS (\n",
    "        SELECT \n",
    "            patientId,\n",
    "            encounterId,\n",
    "            procedureCode,\n",
    "            COUNT(*) AS procedureCount,\n",
    "            ROW_NUMBER() OVER (PARTITION BY encounterId ORDER BY COUNT(*) DESC) AS rank\n",
    "        FROM ICD10procedure\n",
    "        GROUP BY patientId, encounterId, procedureCode\n",
    "    )\n",
    "    SELECT\n",
    "        patientId,\n",
    "        encounterId,\n",
    "        MAX(CASE WHEN rank = 1 THEN procedureCode END) AS procedure_1,\n",
    "        MAX(CASE WHEN rank = 2 THEN procedureCode END) AS procedure_2,\n",
    "        MAX(CASE WHEN rank = 3 THEN procedureCode END) AS procedure_3\n",
    "    FROM Rankedprocedures\n",
    "    GROUP BY patientId, encounterId\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# procedure_pivot_df.write.mode(\"overwrite\").parquet(\"_output/fm2_procedure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Consolidated Encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_encounter = spark.read.parquet(\"_output/fm_encounter/\")\n",
    "fm_encounter.createOrReplaceTempView('fm_encounter')\n",
    "\n",
    "fm_patient = spark.read.parquet(\"_output/fm_patient/\")\n",
    "fm_patient.createOrReplaceTempView('fm_patient')\n",
    "\n",
    "fm2_condition = spark.read.parquet(\"_output/fm2_condition/\")\n",
    "fm2_condition.createOrReplaceTempView('fm2_condition')\n",
    "\n",
    "fm2_procedure = spark.read.parquet(\"_output/fm2_procedure/\")\n",
    "fm2_procedure.createOrReplaceTempView('fm2_procedure')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "encounter_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    encounter.encounterId,\n",
    "    encounter.patientId,\n",
    "    patient.gender,\n",
    "    CASE \n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 18 AND 29 THEN 'young adults'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 30 AND 39 THEN 'young adulthood'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 40 AND 49 THEN 'early-middle age'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 50 AND 59 THEN 'late-middle age'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 60 AND 69 THEN 'mid-old age'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 70 AND 79 THEN 'senior-old age'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 80 AND 89 THEN 'very senior-old'\n",
    "        WHEN ROUND(datediff(patient.birthDate, encounter.periodStart)/365, 0) BETWEEN 90 AND 115 THEN 'centenarians'\n",
    "        ELSE 'other age groups'\n",
    "    END AS ageGroup,\n",
    "    patient.maritalStatus,\n",
    "    encounter.duration encounterDuration,\n",
    "    encounter.status encounterStatus,\n",
    "    encounter.encounterClass,\n",
    "    encounter.displayType encounterType,\n",
    "    encounter.priority,\n",
    "    -- Conditions\n",
    "    condition.condition_1,\n",
    "    condition.condition_2,\n",
    "    condition.condition_3,\n",
    "    -- Procedures\n",
    "    procedure.procedure_1,\n",
    "    procedure.procedure_2,\n",
    "    procedure.procedure_3,\n",
    "    encounter.readmissionStatus,\n",
    "    encounter.sourceName\n",
    "FROM fm_encounter encounter\n",
    "LEFT JOIN fm_patient patient\n",
    "    ON encounter.patientId = patient.patientID\n",
    "LEFT JOIN fm2_condition condition\n",
    "    ON encounter.encounterId = condition.encounterId\n",
    "    AND encounter.patientID = condition.patientID\n",
    "LEFT JOIN fm2_procedure procedure\n",
    "    ON encounter.encounterId = procedure.encounterId\n",
    "    AND encounter.patientID = procedure.patientID\n",
    "\"\"\")\n",
    "# encounter_df.write.mode(\"overwrite\").parquet(\"_output/encounter\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
